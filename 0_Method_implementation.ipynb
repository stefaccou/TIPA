{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bc84e9d77696c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "Development notebook for the TIPA methodology.\n",
    "It contains mainly experiments on isolating the correct parts of the language adapters for subsequent informed averaging.\n",
    "On the other hand, there is some experimentation to retrieve inconsistent ISO-glottolog mappings and find some of the languages that need exceptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e387e2aaeaffa02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:48.461162100Z",
     "start_time": "2025-05-01T12:15:44.648404500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, Stack\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"xlm-roberta-base\")\n",
    "# we load in two adapters\n",
    "model.load_adapter(\"AdapterHub/xlm-roberta-base-de-wiki_pfeiffer\", load_as=\"de\")\n",
    "model.load_adapter(\"AdapterHub/xlm-roberta-base-en-wiki_pfeiffer\", load_as=\"en\")\n",
    "model.load_adapter(\"AdapterHub/xlm-roberta-base-es-wiki_pfeiffer\", load_as=\"es\")\n",
    "# model.load_adapter(\"./trained_adapters/family/en-de-nl-af/mlm\", load_as=\"fam\")\n",
    "\n",
    "model.active_adapters = Stack(\"de\", \"es\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4978e09ceb57c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:48.923102400Z",
     "start_time": "2025-05-01T12:15:48.872910900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sd = model.state_dict()\n",
    "organized_layers = {}\n",
    "# for each layer:\n",
    "# group 1: layer number\n",
    "# group 2: adapter name\n",
    "# group 3: projection\n",
    "# group 4: projection weight/bias\n",
    "pattern = \"roberta\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\"\n",
    "\n",
    "inv_adapters = {}\n",
    "# For invertible adapters\n",
    "# group 1: adapter name\n",
    "# group 2: F/G identifier\n",
    "# group 3: 0/2 layer number\n",
    "# group 4: projection weight/bias\n",
    "inv_pattern = \"roberta\\.invertible_adapters\\.(\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\"\n",
    "for key in model.state_dict().keys():\n",
    "    match = re.search(pattern, key)\n",
    "    if match:\n",
    "        layer_num = str(match.group(1))\n",
    "        if layer_num not in organized_layers:\n",
    "            organized_layers[layer_num] = {}\n",
    "        adapter_name = match.group(2)\n",
    "        projection = match.group(3)\n",
    "        projection_type = match.group(4)\n",
    "        # print(f\"Layer: {layer_num}, Adapter: {adapter_name}, Projection: {projection}, Type: {projection_type}\")\n",
    "        if projection not in organized_layers[layer_num]:\n",
    "            organized_layers[layer_num][projection] = {}\n",
    "        if projection_type not in organized_layers[layer_num][projection]:\n",
    "            organized_layers[layer_num][projection][projection_type] = []\n",
    "        organized_layers[layer_num][projection][projection_type].append(key)\n",
    "    inv_match = re.search(inv_pattern, key)\n",
    "    if inv_match:\n",
    "        adapter_name = inv_match.group(1)\n",
    "        identifier = inv_match.group(2)\n",
    "        layer_num = inv_match.group(3)\n",
    "        projection_type = inv_match.group(4)\n",
    "        if identifier not in inv_adapters:\n",
    "            inv_adapters[identifier] = {}\n",
    "        if layer_num not in inv_adapters[identifier]:\n",
    "            inv_adapters[identifier][layer_num] = {}\n",
    "        if projection_type not in inv_adapters[identifier][layer_num]:\n",
    "            inv_adapters[identifier][layer_num][projection_type] = []\n",
    "        inv_adapters[identifier][layer_num][projection_type].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f65f2b3dd8cadf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:51.336206800Z",
     "start_time": "2025-05-01T12:15:51.311066100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# we now average the weights and biases of all layers over all adapters\n",
    "new_state_dict = OrderedDict()\n",
    "# to ensure we don't get problems, we check the config of all adapters\n",
    "all_adapters = list(model.adapters_config.adapters.keys())\n",
    "config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "config = model.adapters_config.config_map[config_id]\n",
    "for i in range(1, len(all_adapters)):\n",
    "    config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "    config_i = model.adapters_config.config_map[config_id]\n",
    "    assert config == config_i, (\n",
    "        f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "    )\n",
    "\n",
    "# if no problem, we go to the next step\n",
    "for layer_num, projections in organized_layers.items():\n",
    "    for projection, types in projections.items():\n",
    "        for projection_type, keys in types.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_weight = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = avg_weight\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = avg_weight\n",
    "\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_bias = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = avg_bias\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = avg_bias\n",
    "for identifier, layer_num in inv_adapters.items():\n",
    "    for layer_num, projections in layer_num.items():\n",
    "        for projection_type, keys in projections.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_weight = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = avg_weight\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_bias = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = avg_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4f9dcba03239b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:16.080286200Z",
     "start_time": "2025-04-25T08:57:15.932158300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have config saved from the last step, we create a new one in the same form\n",
    "if \"joined_adapter\" in model.adapters_config.adapters.keys():\n",
    "    # remove the old one\n",
    "    model.delete_adapter(\"joined_adapter\")\n",
    "model.add_adapter(\"joined_adapter\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bfa3a168ce9b9df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:23.271574400Z",
     "start_time": "2025-04-25T08:57:23.226464300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "    if \"joined_adapter\" in name and name in new_state_dict:\n",
    "        param.data.copy_(new_state_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "339ea2934154139d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:29.222230100Z",
     "start_time": "2025-04-25T08:57:29.130983500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleDict(\n  (joined_adapter): NICECouplingBlock(\n    (F): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n    (G): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in list(model.adapters_config.adapters.keys()):\n",
    "    if key != \"joined_adapter\":\n",
    "        model.delete_adapter(key)\n",
    "model.roberta.invertible_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ef990023bdf7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:18:27.706772400Z",
     "start_time": "2025-04-25T08:18:27.640331100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"./trained_adapters/mono/huge_avg_adapter\", \"huge_avg_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c10add83e138ca23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:41.801435Z",
     "start_time": "2025-04-23T20:14:41.796737500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we evaluated the adapter (along with de and en) on ner in another script\n",
    "import json\n",
    "\n",
    "results = json.load(open(\"methods/eval_dict_joined.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116e1fa2605c1c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:46.511129300Z",
     "start_time": "2025-04-23T20:14:46.507374900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss, avg en/de: 0.467184379696846, joined: 0.4572905898094177\n",
      "eval_model_preparation_time, avg en/de: 0.0086, joined: 0.006\n",
      "eval_precision, avg en/de: 0.5284809848704373, joined: 0.5575268817204301\n",
      "eval_recall, avg en/de: 0.7186234817813766, joined: 0.6997300944669366\n",
      "eval_f1, avg en/de: 0.6085899656003557, joined: 0.6205864751645721\n",
      "eval_accuracy, avg en/de: 0.8567322573513155, joined: 0.8606566438204731\n",
      "eval_runtime, avg en/de: 4.626099999999999, joined: 4.6817\n",
      "eval_samples_per_second, avg en/de: 216.16500000000002, joined: 213.599\n",
      "eval_steps_per_second, avg en/de: 27.0205, joined: 26.7\n"
     ]
    }
   ],
   "source": [
    "for (name, de), (_, en), (_, joined) in zip(\n",
    "    results[\"de\"].items(), results[\"en\"].items(), results[\"joined_adapter\"].items()\n",
    "):\n",
    "    print(f\"{name}, avg en/de: {(en + de) / 2}, joined: {joined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f7ecc3812d3b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:08:58.016222400Z",
     "start_time": "2025-04-25T08:08:57.665399600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# Fetch all AdapterHub xlm-roberta-base adapters\n",
    "models = api.list_models(author=\"AdapterHub\", library=\"adapter-transformers\", search=\"xlm-roberta-base-\")\n",
    "# we print all found models\n",
    "\n",
    "to_load = {\n",
    "    m.modelId: m.modelId.split(\"xlm-roberta-base-\")[-1].rsplit(\"-wiki_pfeiffer\", 1)[0]\n",
    "    for m in models\n",
    "    if m.modelId.startswith(\"AdapterHub/xlm-roberta-base-\") and m.modelId.endswith(\"-wiki_pfeiffer\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8300369a4d6da78f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:38:19.851726700Z",
     "start_time": "2025-04-25T08:38:02.071706200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452e37b101c242a79ccc9e96edfdd0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274360ba76104d27b3111c9e2c6f6060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9988603b604433be5c6882f581f60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12c90b2c7dc48f5ab67340774604126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435a841d8b824c7db839c214e4d73f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5709fe7b45c440e4af905da94df8789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7de4b1634f4c77a82bae61c59317a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d87d38da3b045b0b15ad3d243d15add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbc121e199c46f2a3afe96ffdd0c1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ffc85411e544c795048c0a604132f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0c5d00792b4dbabaea37c414074e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d7ae558d004eb3828fdffa767f8881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c7656e99d1467e8e8212608f510185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e56970c81642a0a920318c99d60e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f22bcc6c9a48aa9f704c41c2a740f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3a7b79db074625a6c0c030d1c68f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f31891bdc64f098dc8b7d6192145b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40b1b5667cd45c2ba29cf40ec1e4172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13773252e60248e69e07b36d64a2359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b79fa02e5b241e1b44689a914d82701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3e438b9fac482ea22f0439e0d3c869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7178c091d0fc4ba192642037d810a799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aee9b766a624ec6a8aaf9c287a42620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce91d3fde3ff4525a2d64c4069cd87a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c46f7c7f6949eaa4b984cbe6cdd9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751233abdc5046a09286f8bf23ab0f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990b77bfa51a4113a5676c840e4a8657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c3d79e2204dd7bb16bbd31e53d3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32280db6310849ce9459c1a6a67701c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, Stack\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"xlm-roberta-base\")\n",
    "didnt_load = []\n",
    "for link, id in to_load.items():\n",
    "    try:\n",
    "        model.load_adapter(link, load_as=id)\n",
    "    except OSError:\n",
    "        print(f\"Could not load {link}\")\n",
    "        didnt_load.append(link)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cbaa3c45636de66",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def merge_loaded_adapters(model, merge_adapter_name=\"joined_adapter\"):\n",
    "    def check_compatibility():\n",
    "        # to ensure we don't get problems, we check the config of all adapters\n",
    "        all_adapters = list(model.adapters_config.adapters.keys())\n",
    "        config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "        config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "        for i in range(1, len(all_adapters)):\n",
    "            config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "            config_i = model.adapters_config.config_map[config_id]\n",
    "            assert config == config_i, (\n",
    "                f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "            )\n",
    "\n",
    "    check_compatibility()\n",
    "\n",
    "    organized_layers = {}\n",
    "    # for each layer:\n",
    "    # group 1: layer number\n",
    "    # group 2: adapter name\n",
    "    # group 3: projection\n",
    "    # group 4: projection weight/bias\n",
    "    pattern = \"roberta\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\"\n",
    "\n",
    "    inv_adapters = {}\n",
    "    # For invertible adapters\n",
    "    # group 1: adapter name\n",
    "    # group 2: F/G identifier\n",
    "    # group 3: 0/2 layer number\n",
    "    # group 4: projection weight/bias\n",
    "    inv_pattern = \"roberta\\.invertible_adapters\\.(\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\"\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        layer_match = re.search(pattern, key)\n",
    "        if layer_match:\n",
    "            layer_num = layer_match.group(1)\n",
    "            # adapter_name = layer_match.group(2)\n",
    "            projection = layer_match.group(3)\n",
    "            projection_type = layer_match.group(4)\n",
    "            # print(f\"Layer: {layer_num}, Adapter: {adapter_name}, Projection: {projection}, Type: {projection_type}\")\n",
    "            if layer_num not in organized_layers:\n",
    "                organized_layers[layer_num] = {}\n",
    "            if projection not in organized_layers[layer_num]:\n",
    "                organized_layers[layer_num][projection] = {}\n",
    "            if projection_type not in organized_layers[layer_num][projection]:\n",
    "                organized_layers[layer_num][projection][projection_type] = []\n",
    "            organized_layers[layer_num][projection][projection_type].append(key)\n",
    "\n",
    "        inv_match = re.search(inv_pattern, key)\n",
    "        if inv_match:\n",
    "            # adapter_name = inv_match.group(1)\n",
    "            identifier = inv_match.group(2)\n",
    "            layer_num = inv_match.group(3)\n",
    "            projection_type = inv_match.group(4)\n",
    "            if identifier not in inv_adapters:\n",
    "                inv_adapters[identifier] = {}\n",
    "            if layer_num not in inv_adapters[identifier]:\n",
    "                inv_adapters[identifier][layer_num] = {}\n",
    "            if projection_type not in inv_adapters[identifier][layer_num]:\n",
    "                inv_adapters[identifier][layer_num][projection_type] = []\n",
    "            inv_adapters[identifier][layer_num][projection_type].append(key)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    # if no problem, we go to the next step\n",
    "    for layer_num, projections in organized_layers.items():\n",
    "        for projection, types in projections.items():\n",
    "            for projection_type, keys in types.items():\n",
    "                result = sum([sd[key] for key in keys]) / len(keys)\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = result\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = result\n",
    "\n",
    "    for identifier, layer_num in inv_adapters.items():\n",
    "        for layer_num, projections in layer_num.items():\n",
    "            for projection_type, keys in projections.items():\n",
    "                result = sum([sd[key] for key in keys]) / len(keys)\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = result\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "37dc81ad7cf9eba6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def improved_merge_loaded_adapters(\n",
    "    model, merge_adapter_name=\"joined_adapter\", weights=None, delete_other=False, patterns=False, model_type=\"roberta\"\n",
    "):\n",
    "    # to ensure we don't get problems, we check the config of all adapters\n",
    "    all_adapters = list(model.adapters_config.adapters.keys())\n",
    "    config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "    config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "    for i in range(1, len(all_adapters)):\n",
    "        config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "        config_i = model.adapters_config.config_map[config_id]\n",
    "        assert config == config_i, (\n",
    "            f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "        )\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1 / len(all_adapters)] * len(all_adapters)\n",
    "    if len(weights) != len(all_adapters):\n",
    "        raise ValueError(f\"Weights length {len(weights)} does not match number of adapters {len(all_adapters)}\")\n",
    "\n",
    "    if not patterns:\n",
    "        patterns = [\n",
    "            f\"{model_type}\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(?:\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\",\n",
    "            f\"{model_type}\\.invertible_adapters\\.(?:\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\",\n",
    "        ]\n",
    "    comp_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    organized_layers = {}\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        # we make a dictionary for each pattern\n",
    "        organized_layers[i] = {}\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        for i, pattern in enumerate(comp_patterns):\n",
    "            match = re.search(pattern, key)\n",
    "            if match:\n",
    "                one = match.group(1)\n",
    "                two = match.group(2)\n",
    "                three = match.group(3)\n",
    "                if one not in organized_layers[i]:\n",
    "                    organized_layers[i][one] = {}\n",
    "                if two not in organized_layers[i][one]:\n",
    "                    organized_layers[i][one][two] = {}\n",
    "                if three not in organized_layers[i][one][two]:\n",
    "                    organized_layers[i][one][two][three] = []\n",
    "                organized_layers[i][one][two][three].append(key)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    for i, one in organized_layers.items():\n",
    "        for one, two in one.items():\n",
    "            for two, three in two.items():\n",
    "                for three, keys in three.items():\n",
    "                    result = sum([sd[key] * weights[j] for j, key in enumerate(keys)])\n",
    "                    if two == \"adapter_down\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.0.{three}\"\n",
    "                        ] = result\n",
    "                    elif two == \"adapter_up\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.{three}\"\n",
    "                        ] = result\n",
    "                    else:\n",
    "                        # we are in the second pattern\n",
    "                        new_state_dict[f\"{model_type}.invertible_adapters.{merge_adapter_name}.{one}.{two}.{three}\"] = (\n",
    "                            result\n",
    "                        )\n",
    "\n",
    "    # we now load in the new model\n",
    "    if merge_adapter_name in model.adapters_config.adapters.keys():\n",
    "        # remove the old one\n",
    "        model.delete_adapter(merge_adapter_name)\n",
    "    model.add_adapter(merge_adapter_name, config=config)\n",
    "    for name, param in model.named_parameters():\n",
    "        # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "        if merge_adapter_name in name and name in new_state_dict:\n",
    "            param.data.copy_(new_state_dict[name])\n",
    "    if delete_other:\n",
    "        for key in list(model.adapters_config.adapters.keys()):\n",
    "            if key != merge_adapter_name:\n",
    "                model.delete_adapter(key)\n",
    "\n",
    "    # no need to return anything as the model is changed in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "46d4351398feeb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:43:58,490 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n",
      "2025-04-25 16:43:58,713 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model1 = copy.deepcopy(model.cpu())\n",
    "improved_merge_loaded_adapters(model1, delete_other=True)\n",
    "new_state_dict = merge_loaded_adapters(model)\n",
    "# we have config saved from the last step, we create a new one in the same form\n",
    "if \"joined_adapter\" in model.adapters_config.adapters.keys():\n",
    "    # remove the old one\n",
    "    model.delete_adapter(\"joined_adapter\")\n",
    "model.add_adapter(\"joined_adapter\", config=config)\n",
    "for name, param in model.named_parameters():\n",
    "    # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "    if \"joined_adapter\" in name and name in new_state_dict:\n",
    "        param.data.copy_(new_state_dict[name])\n",
    "for key in list(model.adapters_config.adapters.keys()):\n",
    "    if key != \"joined_adapter\":\n",
    "        model.delete_adapter(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4f1ef188a1bafe3f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are equal\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sd1 = model1.state_dict()\n",
    "sd2 = model.state_dict()\n",
    "for key in sd1.keys():\n",
    "    if key in sd2.keys():\n",
    "        assert torch.equal(sd1[key], sd2[key]), f\"Key {key} is not equal\"\n",
    "print(\"Models are equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "68042516a55ec3b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:13:18.333193200Z",
     "start_time": "2025-04-25T13:12:46.562082Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[117], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m glot \u001B[38;5;129;01min\u001B[39;00m not_found:\n\u001B[0;32m     21\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://glottolog.org/resource/languoid/id/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mglot\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 22\u001B[0m     response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(url)\n\u001B[0;32m     23\u001B[0m     soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# find the first h1 tag\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[0;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[0;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    679\u001B[0m     )\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    790\u001B[0m     conn,\n\u001B[0;32m    791\u001B[0m     method,\n\u001B[0;32m    792\u001B[0m     url,\n\u001B[0;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[0;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[0;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[0;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[0;32m    802\u001B[0m )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# Trigger any extra validation we need to do.\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 466\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_conn(conn)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mconn\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[1;34m(self, conn)\u001B[0m\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;66;03m# Force connect early to allow us to validate the connection.\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[1;32m-> 1095\u001B[0m     conn\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001B[39;00m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mproxy_is_verified:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connection.py:693\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    692\u001B[0m     sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[1;32m--> 693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_conn()\n\u001B[0;32m    694\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[0;32m    695\u001B[0m     tls_in_tls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \n\u001B[0;32m    196\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[0;32m    200\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dns_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport),\n\u001B[0;32m    201\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout,\n\u001B[0;32m    202\u001B[0m         source_address\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address,\n\u001B[0;32m    203\u001B[0m         socket_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket_options,\n\u001B[0;32m    204\u001B[0m     )\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001B[0m, in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeError\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocationParseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, label empty or too long\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgetaddrinfo(host, port, family, socket\u001B[38;5;241m.\u001B[39mSOCK_STREAM):\n\u001B[0;32m     61\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[0;32m     62\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\socket.py:974\u001B[0m, in \u001B[0;36mgetaddrinfo\u001B[1;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;66;03m# We override this function since we want to translate the numeric family\u001B[39;00m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# and socket type values to enum constants.\u001B[39;00m\n\u001B[0;32m    973\u001B[0m addrlist \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 974\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m _socket\u001B[38;5;241m.\u001B[39mgetaddrinfo(host, port, family, \u001B[38;5;28mtype\u001B[39m, proto, flags):\n\u001B[0;32m    975\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[0;32m    976\u001B[0m     addrlist\u001B[38;5;241m.\u001B[39mappend((_intenum_converter(af, AddressFamily),\n\u001B[0;32m    977\u001B[0m                      _intenum_converter(socktype, SocketKind),\n\u001B[0;32m    978\u001B[0m                      proto, canonname, sa))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from qq import LanguageData, TagType\n",
    "from typdiv_sampling.evaluation import Evaluator\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "ld = LanguageData.from_db()\n",
    "\n",
    "evaluator = Evaluator()\n",
    "distances = evaluator.distances\n",
    "\n",
    "all_english = {}\n",
    "not_found = []\n",
    "for key in distances.keys():\n",
    "    try:\n",
    "        all_english[ld.get(key, tag_type=TagType.Glottocode).english_name] = key\n",
    "    except KeyError:\n",
    "        not_found.append(key)\n",
    "        continue\n",
    "\n",
    "for glot in not_found:\n",
    "    url = f\"https://glottolog.org/resource/languoid/id/{glot}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # find the first h1 tag\n",
    "    h3 = soup.find(\"h3\")\n",
    "    if h3:\n",
    "        span = h3.find(\"span\")\n",
    "        if span:\n",
    "            # print(span.text)  # This will print only the language name\n",
    "            all_english[span.text] = glot\n",
    "        else:\n",
    "            print(\"No <span> with class 'level-language' found.\")\n",
    "            print(h3.text)  # This will print the entire h3 text\n",
    "    else:\n",
    "        print(\"No <h3> tag found.\")\n",
    "\n",
    "\n",
    "def lookup_lang(snippet):\n",
    "    results = []\n",
    "    for lang in all_english.keys():\n",
    "        if re.search(snippet, lang):\n",
    "            results.append((lang, all_english[lang]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95b90d9ba2788e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:13:21.711707700Z",
     "start_time": "2025-04-25T13:13:21.704708700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ('Congo Swahili', 'cong1236') for Swahili\n",
      "Found ('Ayacucho Quechua', 'ayac1239') for Quechua\n",
      "Error: de - German - stan1295\n",
      "Found ('Standard Estonian', 'esto1258') for Estonian\n",
      "Found ('Eastern Bolivian Guaran', 'east2555') for Guarani\n",
      "Found ('Baharna Arabic', 'baha1259') for Arabic\n",
      "Error: es - Spanish - stan1288\n",
      "Found ('Hakka Chinese', 'hakk1236') for Chinese\n",
      "Error: mhr - Eastern Mari - east2328\n",
      "Error: cdo - Min Dong Chinese - mind1253\n",
      "Error: xmf - Mingrelian - ming1252\n",
      "{'ar': 'baha1259',\n",
      " 'el': 'mode1248',\n",
      " 'en': 'stan1293',\n",
      " 'et': 'esto1258',\n",
      " 'gn': 'east2555',\n",
      " 'hi': 'hind1269',\n",
      " 'ht': 'hait1244',\n",
      " 'id': 'indo1316',\n",
      " 'ilo': 'ilok1237',\n",
      " 'is': 'icel1247',\n",
      " 'it': 'ital1282',\n",
      " 'ja': 'nucl1643',\n",
      " 'jv': 'java1254',\n",
      " 'mi': 'maor1246',\n",
      " 'my': 'nucl1310',\n",
      " 'qu': 'ayac1239',\n",
      " 'ru': 'russ1263',\n",
      " 'sw': 'cong1236',\n",
      " 'ta': 'tami1289',\n",
      " 'th': 'thai1261',\n",
      " 'tk': 'turk1304',\n",
      " 'tr': 'nucl1301',\n",
      " 'vi': 'viet1252',\n",
      " 'zh': 'hakk1236'}\n",
      "[('de', 'German', 'stan1295'), ('es', 'Spanish', 'stan1288'), ('mhr', 'Eastern Mari', 'east2328'), ('cdo', 'Min Dong Chinese', 'mind1253'), ('xmf', 'Mingrelian', 'ming1252')]\n"
     ]
    }
   ],
   "source": [
    "manuals = {\n",
    "    \"Arabic\": \"arab1267\",\n",
    "    \"Swahili\": \"swah1253\",\n",
    "    \"Bengali\": \"beng1282\",\n",
    "    \"Chinese\": \"mand1415\",\n",
    "    \"Persian\": \"west2369\",\n",
    "    \"Yoruba\": \"ilaa1246\",\n",
    "    \"Nepali\": \"nepa1254\",\n",
    "    \"Quechua\": \"cusc1236\",\n",
    "    \"Estonian\": \"esto1258\",\n",
    "    \"Guarani\": \"east2555\",\n",
    "}\n",
    "\n",
    "glots = {}\n",
    "probs = []\n",
    "\n",
    "for lang in to_load.values():\n",
    "    eng = ld.get(lang, tag_type=TagType.BCP_47_CODE).english_name\n",
    "    glot = ld.get(lang, tag_type=TagType.BCP_47_CODE).glottocode\n",
    "    # we need to find if glot is in distances\n",
    "    if not glot:\n",
    "        options = lookup_lang(eng[:-1])\n",
    "        # print(options)\n",
    "        if options:\n",
    "            verb_name, glot = options[0]\n",
    "            print(f\"Found {verb_name, glot} for {eng}\")\n",
    "            glots[lang] = glot\n",
    "        else:\n",
    "            print(f\"Error for {lang}: {eng} - {glot}\")\n",
    "            probs.append((lang, eng, glot))\n",
    "            continue\n",
    "    if glot not in distances.keys():\n",
    "        print(f\"Error: {lang} - {eng} - {glot}\")\n",
    "        probs.append((lang, eng, glot))\n",
    "        continue\n",
    "    if eng and glot:\n",
    "        glots[lang] = glot\n",
    "\n",
    "pprint(glots)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5d74f7272b6c66aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:35:32.934530200Z",
     "start_time": "2025-04-25T14:35:15.431742700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting to URIEL involves copying the files ['family_features.npz', 'features.npz', 'geocoord_features.npz'] into the data directory. Any files with the same name will be replaced. Continue? [Y/n] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:35:20,332 - root - INFO - Importing all databases....\n",
      "2025-04-25 16:35:20,335 - root - INFO - Importing updated SAPHON from \"saphon_data.csv\"....\n",
      "2025-04-25 16:35:21,840 - root - INFO - Updated SAPHON integration complete..\n",
      "2025-04-25 16:35:21,842 - root - INFO - Importing BDPROTO from \"bdproto_data.csv\"....\n",
      "2025-04-25 16:35:21,843 - root - INFO - Converting ISO 639-3 codes to Glottocodes....\n",
      "2025-04-25 16:35:22,236 - root - INFO - Conversion to Glottocodes complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[200], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m u\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m      4\u001B[0m u\u001B[38;5;241m.\u001B[39mset_cache(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 5\u001B[0m u\u001B[38;5;241m.\u001B[39mintegrate_databases()\n\u001B[0;32m      6\u001B[0m u\u001B[38;5;241m.\u001B[39mset_glottocodes()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:596\u001B[0m, in \u001B[0;36mURIELPlusDatabases.integrate_databases\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m db, integrate_method \u001B[38;5;129;01min\u001B[39;00m databases\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m db \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msources[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 596\u001B[0m         integrate_method()\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minferred_features()\n\u001B[0;32m    602\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll databases integration complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:364\u001B[0m, in \u001B[0;36mURIELPlusDatabases.integrate_bdproto\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache:\n\u001B[0;32m    360\u001B[0m     np\u001B[38;5;241m.\u001B[39msavez(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcur_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabase\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles[\u001B[38;5;241m1\u001B[39m]),\n\u001B[0;32m    361\u001B[0m              feats\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeats[\u001B[38;5;241m1\u001B[39m], data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m1\u001B[39m], langs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m1\u001B[39m], sources\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msources[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m--> 364\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_phylogeny_vectors()\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_geocoord_vectors()\n\u001B[0;32m    368\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBDPROTO integration complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:134\u001B[0m, in \u001B[0;36mURIELPlusDatabases._calculate_phylogeny_vectors\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m], l)\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]))])\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m], l)\n\u001B[0;32m    136\u001B[0m new_lang_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m l)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\numpy\\_core\\shape_base.py:291\u001B[0m, in \u001B[0;36mvstack\u001B[1;34m(tup, dtype, casting)\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arrs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    290\u001B[0m     arrs \u001B[38;5;241m=\u001B[39m (arrs,)\n\u001B[1;32m--> 291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _nx\u001B[38;5;241m.\u001B[39mconcatenate(arrs, \u001B[38;5;241m0\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype, casting\u001B[38;5;241m=\u001B[39mcasting)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from urielplus import urielplus\n",
    "\n",
    "u = urielplus.URIELPlus()\n",
    "u.reset()\n",
    "u.set_cache(True)\n",
    "u.integrate_databases()\n",
    "u.set_glottocodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7090d376cb17f78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:46:10.392463Z",
     "start_time": "2025-04-25T14:46:10.359076100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def typological_approximation_dev(target, languages):\n",
    "    \"\"\"\n",
    "    This function takes a target language and a list of languages.\n",
    "    It weights the other languages depending on their closeness to the target language.\n",
    "    \"\"\"\n",
    "    # 1. check if all languages are in the distances\n",
    "    for lang in languages:\n",
    "        if glots[lang] not in distances.keys():\n",
    "            print(f\"Language {lang}, {glots[lang]} not in distances\")\n",
    "\n",
    "    # 2. retrieve closeness score of all languages to target language\n",
    "    weights = []\n",
    "    for lang in languages:\n",
    "        # get the distance\n",
    "        try:\n",
    "            dist = 1 - u.new_distance(\"featural\", [glots[lang], target])\n",
    "            print(f\"Distance {lang} to {target}: {dist}\")\n",
    "        except SystemExit:\n",
    "            print(f\"Error: {lang} - {glots[lang]} - {target}\")\n",
    "            dist = 0\n",
    "        weights.append(dist)\n",
    "\n",
    "    # 1. softmax over weights\n",
    "    print(f\"Weights before softmax: {weights}\")\n",
    "    weights = torch.softmax(torch.tensor(weights), dim=0)\n",
    "    # we need to convert to list\n",
    "    weights = weights.tolist()\n",
    "    print(f\"Weights after softmax: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e16022d51bc3b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "588490bb28529704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:48:23.437832800Z",
     "start_time": "2025-04-25T14:48:23.094231800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:48:23,105 - root - INFO - In new_distance, calculated angular distance for featural with thai1261 and afri1274: 0.011420488357543945 seconds\n",
      "2025-04-25 16:48:23,119 - root - INFO - In new_distance, calculated angular distance for featural with nucl1310 and afri1274: 0.010793685913085938 seconds\n",
      "2025-04-25 16:48:23,129 - root - INFO - In new_distance, calculated angular distance for featural with hind1269 and afri1274: 0.008425235748291016 seconds\n",
      "2025-04-25 16:48:23,141 - root - INFO - In new_distance, calculated angular distance for featural with ilok1237 and afri1274: 0.009675741195678711 seconds\n",
      "2025-04-25 16:48:23,150 - root - INFO - In new_distance, calculated angular distance for featural with hait1244 and afri1274: 0.00613713264465332 seconds\n",
      "2025-04-25 16:48:23,160 - root - INFO - In new_distance, calculated angular distance for featural with nucl1301 and afri1274: 0.008888959884643555 seconds\n",
      "2025-04-25 16:48:23,172 - root - INFO - In new_distance, calculated angular distance for featural with maor1246 and afri1274: 0.009157180786132812 seconds\n",
      "2025-04-25 16:48:23,186 - root - INFO - In new_distance, calculated angular distance for featural with viet1252 and afri1274: 0.010975837707519531 seconds\n",
      "2025-04-25 16:48:23,200 - root - INFO - In new_distance, calculated angular distance for featural with icel1247 and afri1274: 0.011210441589355469 seconds\n",
      "2025-04-25 16:48:23,213 - root - INFO - In new_distance, calculated angular distance for featural with ital1282 and afri1274: 0.010046005249023438 seconds\n",
      "2025-04-25 16:48:23,222 - root - INFO - In new_distance, calculated angular distance for featural with tami1289 and afri1274: 0.007052898406982422 seconds\n",
      "2025-04-25 16:48:23,231 - root - INFO - In new_distance, calculated angular distance for featural with java1254 and afri1274: 0.006459712982177734 seconds\n",
      "2025-04-25 16:48:23,244 - root - INFO - In new_distance, calculated angular distance for featural with nucl1643 and afri1274: 0.010086774826049805 seconds\n",
      "2025-04-25 16:48:23,248 - root - ERROR - No shared featural features between cong1236 and afri1274 for which the two languages have information.\n",
      "Unable to calculate featural distance.\n",
      "2025-04-25 16:48:23,260 - root - INFO - In new_distance, calculated angular distance for featural with ayac1239 and afri1274: 0.00893259048461914 seconds\n",
      "2025-04-25 16:48:23,273 - root - INFO - In new_distance, calculated angular distance for featural with mode1248 and afri1274: 0.01037907600402832 seconds\n",
      "2025-04-25 16:48:23,284 - root - INFO - In new_distance, calculated angular distance for featural with esto1258 and afri1274: 0.008218765258789062 seconds\n",
      "2025-04-25 16:48:23,293 - root - INFO - In new_distance, calculated angular distance for featural with russ1263 and afri1274: 0.007620573043823242 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance th to afri1274: 0.5305\n",
      "Distance my to afri1274: 0.5578\n",
      "Distance hi to afri1274: 0.5374\n",
      "Distance ilo to afri1274: 0.5095\n",
      "Distance ht to afri1274: 0.5662\n",
      "Distance tr to afri1274: 0.4901\n",
      "Distance mi to afri1274: 0.5587\n",
      "Distance vi to afri1274: 0.5451\n",
      "Distance is to afri1274: 0.4266\n",
      "Distance it to afri1274: 0.4516\n",
      "Distance ta to afri1274: 0.5581\n",
      "Distance jv to afri1274: 0.4661\n",
      "Distance ja to afri1274: 0.5726\n",
      "Error: sw - cong1236 - afri1274\n",
      "Distance qu to afri1274: 0.6357\n",
      "Distance el to afri1274: 0.5302\n",
      "Distance et to afri1274: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:48:23,305 - root - INFO - In new_distance, calculated angular distance for featural with east2555 and afri1274: 0.009303569793701172 seconds\n",
      "2025-04-25 16:48:23,316 - root - INFO - In new_distance, calculated angular distance for featural with indo1316 and afri1274: 0.008577585220336914 seconds\n",
      "2025-04-25 16:48:23,328 - root - INFO - In new_distance, calculated angular distance for featural with stan1293 and afri1274: 0.008578062057495117 seconds\n",
      "2025-04-25 16:48:23,333 - root - ERROR - No shared featural features between baha1259 and afri1274 for which the two languages have information.\n",
      "Unable to calculate featural distance.\n",
      "2025-04-25 16:48:23,343 - root - INFO - In new_distance, calculated angular distance for featural with turk1304 and afri1274: 0.007827520370483398 seconds\n",
      "2025-04-25 16:48:23,354 - root - INFO - In new_distance, calculated angular distance for featural with hakk1236 and afri1274: 0.007961750030517578 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance ru to afri1274: 0.4596\n",
      "Distance gn to afri1274: 0.5552\n",
      "Distance id to afri1274: 0.5453\n",
      "Distance en to afri1274: 0.4907\n",
      "Error: ar - baha1259 - afri1274\n",
      "Distance tk to afri1274: 0.3918\n",
      "Distance zh to afri1274: 0.5288\n",
      "Weights before softmax: [np.float64(0.5305), np.float64(0.5578), np.float64(0.5374), np.float64(0.5095), np.float64(0.5662), np.float64(0.4901), np.float64(0.5587), np.float64(0.5451), np.float64(0.4266), np.float64(0.4516), np.float64(0.5581), np.float64(0.4661), np.float64(0.5726), 0, np.float64(0.6357), np.float64(0.5302), np.float64(0.2952), np.float64(0.4596), np.float64(0.5552), np.float64(0.5453), np.float64(0.4907), 0, np.float64(0.3918), np.float64(0.5288)]\n",
      "Weights after softmax: [0.043923663689886254, 0.0451392976126194, 0.04422778498120124, 0.04301088447850758, 0.045520064695405295, 0.0421845150209015, 0.04517994126737195, 0.04456965342997089, 0.03958907558236617, 0.04059127780218449, 0.045152841433374724, 0.041184139187974006, 0.04581232735236898, 0.025840763122152884, 0.04879623757770814, 0.04391048856714651, 0.0347143522541419, 0.04091731041621935, 0.04502208787751038, 0.04457856825210938, 0.04220983332464562, 0.025840763122152884, 0.03823507205672382, 0.0438490568953566]\n"
     ]
    }
   ],
   "source": [
    "weights = typological_approximation_dev(\"afri1274\", list(glots.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf49c09fa9227fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Limiting the activated adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6dec64f6f7b85ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:01.153331700Z",
     "start_time": "2025-05-01T13:33:01.136100600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_loaded_adapters(\n",
    "    model, merge_adapter_name=\"joined_adapter\", weights=None, delete_other=False, patterns=False, model_type=\"roberta\"\n",
    "):\n",
    "    # to ensure we don't get problems, we check the config of all adapters\n",
    "    all_adapters = list(model.adapters_config.adapters.keys())\n",
    "    config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "    config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "    for i in range(1, len(all_adapters)):\n",
    "        config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "        config_i = model.adapters_config.config_map[config_id]\n",
    "        assert config == config_i, (\n",
    "            f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "        )\n",
    "\n",
    "    if weights is None or weights == {}:\n",
    "        weights = {adapter: 1 / len(all_adapters) for adapter in all_adapters}\n",
    "    print(\"weights:\", weights)\n",
    "    if not patterns:\n",
    "        patterns = [\n",
    "            f\"{model_type}\\.encoder\\.layer\\.(?P<one>[\\d\\w]+)\\.output\\.adapters\\.(?P<adapter>\\w+)\\.(?P<two>\\w+)(?:\\.0)?\\.(?P<three>\\w+)\",\n",
    "            f\"{model_type}\\.invertible_adapters\\.(?P<adapter>\\w+)\\.(?P<one>\\w+)\\.(?P<two>\\d)\\.(?P<three>\\w+)\",\n",
    "        ]\n",
    "    comp_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    organized_layers = {}\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        # we make a dictionary for each pattern\n",
    "        organized_layers[i] = {}\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        for i, pattern in enumerate(comp_patterns):\n",
    "            match = re.search(pattern, key)\n",
    "            if match:\n",
    "                one = match.group(\"one\")\n",
    "                two = match.group(\"two\")\n",
    "                three = match.group(\"three\")\n",
    "                adapter_name = match.group(\"adapter\")\n",
    "                if adapter_name not in weights.keys():\n",
    "                    print(f\"Adapter {adapter_name} not in weights\")\n",
    "                    continue\n",
    "                if one not in organized_layers[i]:\n",
    "                    organized_layers[i][one] = {}\n",
    "                if two not in organized_layers[i][one]:\n",
    "                    organized_layers[i][one][two] = {}\n",
    "                if three not in organized_layers[i][one][two]:\n",
    "                    organized_layers[i][one][two][three] = []\n",
    "                organized_layers[i][one][two][three].append((key, adapter_name))\n",
    "    pprint(organized_layers)\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    for i, one in organized_layers.items():\n",
    "        for one, two in one.items():\n",
    "            for two, three in two.items():\n",
    "                for three, keys in three.items():\n",
    "                    # result = sum([sd[layer] * weights[adapter_name] for layer, adapter_name in keys])\n",
    "                    result = 0\n",
    "                    for layer, adapter_name in keys:\n",
    "                        print(layer, adapter_name, weights[adapter_name])\n",
    "                        if adapter_name in weights.keys():\n",
    "                            result += sd[layer] * weights[adapter_name]\n",
    "\n",
    "                    if two == \"adapter_down\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.0.{three}\"\n",
    "                        ] = result\n",
    "                    elif two == \"adapter_up\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.{three}\"\n",
    "                        ] = result\n",
    "                    else:\n",
    "                        # we are in the second pattern\n",
    "                        new_state_dict[f\"{model_type}.invertible_adapters.{merge_adapter_name}.{one}.{two}.{three}\"] = (\n",
    "                            result\n",
    "                        )\n",
    "\n",
    "    # we now load in the new model\n",
    "    if merge_adapter_name in model.adapters_config.adapters.keys():\n",
    "        # remove the old one\n",
    "        model.delete_adapter(merge_adapter_name)\n",
    "    model.add_adapter(merge_adapter_name, config=config)\n",
    "    for name, param in model.named_parameters():\n",
    "        # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "        if merge_adapter_name in name and name in new_state_dict:\n",
    "            param.data.copy_(new_state_dict[name])\n",
    "    if delete_other:\n",
    "        for key in list(model.adapters_config.adapters.keys()):\n",
    "            if key != merge_adapter_name:\n",
    "                model.delete_adapter(key)\n",
    "\n",
    "\n",
    "def typological_approximation(target, glots, limit=None):\n",
    "    \"\"\"\n",
    "    This function takes a target language and a list of languages.\n",
    "    It weights the other languages depending on their closeness to the target language.\n",
    "    If limit is specified and is <1, it will remove all languages with a distance lower than limit.\n",
    "    If limit is specified and is >=1, it works as a top-k languages filter with the highest similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. retrieve closeness score of all languages to target language\n",
    "    weights = {}\n",
    "    for lang, glot in glots.items():\n",
    "        # get the distance\n",
    "        try:\n",
    "            dist = 1 - u.new_distance(\"featural\", [glot, target])\n",
    "            print(f\"Distance {lang} to {target}: {dist}\")\n",
    "        except SystemExit:\n",
    "            print(f\"Error: {lang} - {glot} - {target}\")\n",
    "            dist = 0\n",
    "        weights[lang] = dist\n",
    "    # we add spanish\n",
    "    es_glot = ld.get(\"es\", tag_type=TagType.BCP_47_CODE).glottocode\n",
    "    dist = 1 - u.new_distance(\"featural\", [es_glot, target])\n",
    "    print(f\"Distance Spanish to {target}: {dist}\")\n",
    "    weights[\"es\"] = dist\n",
    "    if limit:\n",
    "        if limit < 1:\n",
    "            for lang, dist in list(weights.items()):\n",
    "                if dist < limit:\n",
    "                    print(f\"Removing {lang} with distance {dist}\")\n",
    "                    del weights[lang]\n",
    "        else:  # we take the best n (limit) languages\n",
    "            n = min(limit, len(weights))\n",
    "            # we sort the weights\n",
    "            sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)\n",
    "            # we take the first n\n",
    "            weights = dict(sorted_weights[:n])\n",
    "\n",
    "    # 1. softmax over weights\n",
    "    print(f\"Weights before softmax: {weights}\")\n",
    "    soft_weights = torch.softmax(torch.tensor(list(weights.values())), dim=0)\n",
    "    # we need to convert to list\n",
    "    soft_weights = soft_weights.tolist()\n",
    "    # we zippedly print the keys and values\n",
    "    weights = {k: v for k, v in zip(weights.keys(), soft_weights)}\n",
    "    print(f\"Weights after softmax: {weights}\")\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_glots(to_load):\n",
    "    manuals = {\n",
    "        \"Arabic\": \"arab1267\",\n",
    "        \"Swahili\": \"swah1253\",\n",
    "        \"Bengali\": \"beng1282\",\n",
    "        \"Chinese\": \"mand1415\",\n",
    "        \"Persian\": \"west2369\",\n",
    "        \"Yoruba\": \"ilaa1246\",\n",
    "        \"Nepali\": \"nepa1254\",\n",
    "        \"Quechua\": \"cusc1236\",\n",
    "        \"Estonian\": \"esto1258\",\n",
    "        \"Guarani\": \"east2555\",\n",
    "    }\n",
    "\n",
    "    glots = {}\n",
    "    probs = []\n",
    "\n",
    "    for lang in to_load.values():\n",
    "        eng = ld.get(lang, tag_type=TagType.BCP_47_CODE).english_name\n",
    "        glot = ld.get(lang, tag_type=TagType.BCP_47_CODE).glottocode\n",
    "        # we need to find if glot is in distances\n",
    "        if not glot:\n",
    "            if eng in manuals.keys():\n",
    "                glot = manuals[eng]\n",
    "        if eng and glot:\n",
    "            glots[lang] = glot\n",
    "        else:\n",
    "            probs.append(lang)\n",
    "\n",
    "    print(\"no glottocodes found for these languages: \", probs)\n",
    "    print(\"removing them from further consideration\")\n",
    "    for prob in probs:\n",
    "        del to_load[prob]\n",
    "        # happens in-place\n",
    "    return glots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "957a4b58a9ff3384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:02.345519900Z",
     "start_time": "2025-05-01T13:33:02.340409100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no glottocodes found for these languages:  []\n",
      "removing them from further consideration\n"
     ]
    }
   ],
   "source": [
    "to_load = {\n",
    "    \"English\": \"en\",\n",
    "    \"German\": \"de\",\n",
    "    \"Spanish\": \"es\",\n",
    "}\n",
    "glots = get_glots(to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a8b6d91b4f1de5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:08:44.122698200Z",
     "start_time": "2025-05-01T14:08:44.070090700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 16:08:44,078 - root - INFO - In new_distance, calculated angular distance for featural with stan1293 and dutc1256: 0.009697198867797852 seconds\n",
      "2025-05-01 16:08:44,086 - root - INFO - In new_distance, calculated angular distance for featural with stan1295 and dutc1256: 0.006561279296875 seconds\n",
      "2025-05-01 16:08:44,094 - root - INFO - In new_distance, calculated angular distance for featural with stan1288 and dutc1256: 0.007330417633056641 seconds\n",
      "2025-05-01 16:08:44,104 - root - INFO - In new_distance, calculated angular distance for featural with basq1248 and dutc1256: 0.008893966674804688 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance en to dutc1256: 0.5473\n",
      "Distance de to dutc1256: 0.6954\n",
      "Distance es to dutc1256: 0.46630000000000005\n",
      "Distance Basque to dutc1256: 0.393\n",
      "Weights before softmax: {'de': np.float64(0.6954), 'en': np.float64(0.5473), 'es': np.float64(0.46630000000000005)}\n",
      "Weights after softmax: {'de': 0.3762802161893739, 'en': 0.3244833164247225, 'es': 0.29923646738590365}\n",
      "Best adapter: de\n"
     ]
    }
   ],
   "source": [
    "weights = typological_approximation(\"dutc1256\", glots, limit=3)\n",
    "sorted_ad = max(weights.items(), key=lambda x: x[1])\n",
    "closest_adapter = max(weights, key=weights.get)\n",
    "print(f\"Best adapter: {closest_adapter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a7d1ca7fae59d19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:09.769830800Z",
     "start_time": "2025-05-01T13:33:09.616229800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 15:33:09,644 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: {'de': 0.3333333333333333, 'en': 0.3333333333333333, 'eu': 0.3333333333333333}\n",
      "{0: {'0': {'adapter_down': {'bias': [('roberta.encoder.layer.0.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.0.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.0.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.0.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.0.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.0.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.0.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '1': {'adapter_down': {'bias': [('roberta.encoder.layer.1.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.1.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.1.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.1.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.1.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.1.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.1.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '10': {'adapter_down': {'bias': [('roberta.encoder.layer.10.output.adapters.de.adapter_down.0.bias',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.en.adapter_down.0.bias',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.bias',\n",
      "                                       'eu')],\n",
      "                             'weight': [('roberta.encoder.layer.10.output.adapters.de.adapter_down.0.weight',\n",
      "                                         'de'),\n",
      "                                        ('roberta.encoder.layer.10.output.adapters.en.adapter_down.0.weight',\n",
      "                                         'en'),\n",
      "                                        ('roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.weight',\n",
      "                                         'eu')]},\n",
      "            'adapter_up': {'bias': [('roberta.encoder.layer.10.output.adapters.de.adapter_up.bias',\n",
      "                                     'de'),\n",
      "                                    ('roberta.encoder.layer.10.output.adapters.en.adapter_up.bias',\n",
      "                                     'en'),\n",
      "                                    ('roberta.encoder.layer.10.output.adapters.eu.adapter_up.bias',\n",
      "                                     'eu')],\n",
      "                           'weight': [('roberta.encoder.layer.10.output.adapters.de.adapter_up.weight',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.en.adapter_up.weight',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.eu.adapter_up.weight',\n",
      "                                       'eu')]}},\n",
      "     '11': {'adapter_down': {'bias': [('roberta.encoder.layer.11.output.adapters.de.adapter_down.0.bias',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.en.adapter_down.0.bias',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.bias',\n",
      "                                       'eu')],\n",
      "                             'weight': [('roberta.encoder.layer.11.output.adapters.de.adapter_down.0.weight',\n",
      "                                         'de'),\n",
      "                                        ('roberta.encoder.layer.11.output.adapters.en.adapter_down.0.weight',\n",
      "                                         'en'),\n",
      "                                        ('roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.weight',\n",
      "                                         'eu')]},\n",
      "            'adapter_up': {'bias': [('roberta.encoder.layer.11.output.adapters.de.adapter_up.bias',\n",
      "                                     'de'),\n",
      "                                    ('roberta.encoder.layer.11.output.adapters.en.adapter_up.bias',\n",
      "                                     'en'),\n",
      "                                    ('roberta.encoder.layer.11.output.adapters.eu.adapter_up.bias',\n",
      "                                     'eu')],\n",
      "                           'weight': [('roberta.encoder.layer.11.output.adapters.de.adapter_up.weight',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.en.adapter_up.weight',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.eu.adapter_up.weight',\n",
      "                                       'eu')]}},\n",
      "     '2': {'adapter_down': {'bias': [('roberta.encoder.layer.2.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.2.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.2.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.2.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.2.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.2.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.2.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '3': {'adapter_down': {'bias': [('roberta.encoder.layer.3.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.3.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.3.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.3.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.3.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.3.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.3.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '4': {'adapter_down': {'bias': [('roberta.encoder.layer.4.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.4.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.4.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.4.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.4.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.4.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.4.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '5': {'adapter_down': {'bias': [('roberta.encoder.layer.5.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.5.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.5.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.5.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.5.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.5.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.5.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '6': {'adapter_down': {'bias': [('roberta.encoder.layer.6.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.6.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.6.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.6.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.6.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.6.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.6.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '7': {'adapter_down': {'bias': [('roberta.encoder.layer.7.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.7.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.7.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.7.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.7.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.7.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.7.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '8': {'adapter_down': {'bias': [('roberta.encoder.layer.8.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.8.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.8.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.8.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.8.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.8.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.8.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '9': {'adapter_down': {'bias': [('roberta.encoder.layer.9.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.9.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.9.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.9.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.9.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.9.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.9.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}}},\n",
      " 1: {'F': {'0': {'bias': [('roberta.invertible_adapters.de.F.0.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.F.0.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.F.0.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.F.0.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.F.0.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.F.0.weight',\n",
      "                             'eu')]},\n",
      "           '2': {'bias': [('roberta.invertible_adapters.de.F.2.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.F.2.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.F.2.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.F.2.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.F.2.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.F.2.weight',\n",
      "                             'eu')]}},\n",
      "     'G': {'0': {'bias': [('roberta.invertible_adapters.de.G.0.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.G.0.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.G.0.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.G.0.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.G.0.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.G.0.weight',\n",
      "                             'eu')]},\n",
      "           '2': {'bias': [('roberta.invertible_adapters.de.G.2.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.G.2.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.G.2.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.G.2.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.G.2.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.G.2.weight',\n",
      "                             'eu')]}}}}\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.0.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.0.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.0.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.0.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.0.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.0.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.2.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.2.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.2.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.2.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.2.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.2.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.0.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.0.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.0.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.0.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.0.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.0.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.2.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.2.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.2.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.2.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.2.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.2.bias eu 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "merge_loaded_adapters(model, merge_adapter_name=\"joined_adapter\", weights=weights)\n",
    "model.delete_adapter(\"joined_adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
