{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bc84e9d77696c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "We check where typological grouping can be most effective,\n",
    "and which type of typological grouping works best.\n",
    "\n",
    "1. Grouping in function aggregation\n",
    "- Phylogeny OR typology inspired stacks\n",
    "- Greater dataset for adapter training\n",
    "2. Parameter aggregation\n",
    "- Arithmetic: typology-informed weights for aggregation\n",
    "3. Representation aggregation\n",
    "- A bit what EMEA does, not efficient at inference time\n",
    "    - EMEA even worse as they \"learn\" at inference\n",
    "\n",
    "## 1. Stacks\n",
    "Train a joint language adapter on a group of languages through MLM\n",
    "Here, a distinction could still be made between:\n",
    "- training jointly, no stack\n",
    "    - equal presence of all languages\n",
    "    - weighted presence of all languages\n",
    "- training jointly in a stack with target language adapter on top\n",
    "    - e.g. We already have a \"Romance\" adapter, train \"Asturian\" adapter on top of this\n",
    "- training jointly with a *changing stack*, activating the adapter for the language batch\n",
    "    - What Faisal does?\n",
    "# 2. Parameter aggregation\n",
    "Arithmetic operations on adapters:\n",
    "- adding existing adapters and compare with jointly trained family adapters\n",
    "    - \"average\" of adapters == jointly trained? (cf. Linear mode connectivity)\n",
    "- re-creating typological profile of a language\n",
    "    - preparation step to then \"fine-tune\" on little data (typologically inspired initialization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e387e2aaeaffa02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:48.461162100Z",
     "start_time": "2025-05-01T12:15:44.648404500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, Stack\n",
    "\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"xlm-roberta-base\")\n",
    "# we load in two adapters\n",
    "model.load_adapter(\"./trained_adapters/mono/de\", load_as=\"de\")\n",
    "model.load_adapter(\"./trained_adapters/mono/en\", load_as=\"en\")\n",
    "model.load_adapter(\"./trained_adapters/mono/eus\", load_as=\"eu\")\n",
    "# model.load_adapter(\"./trained_adapters/family/en-de-nl-af/mlm\", load_as=\"fam\")\n",
    "\n",
    "model.active_adapters = Stack(\"de\", \"eu\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4978e09ceb57c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:48.923102400Z",
     "start_time": "2025-05-01T12:15:48.872910900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sd = model.state_dict()\n",
    "organized_layers = {}\n",
    "# for each layer:\n",
    "# group 1: layer number\n",
    "# group 2: adapter name\n",
    "# group 3: projection\n",
    "# group 4: projection weight/bias\n",
    "pattern = \"roberta\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\"\n",
    "\n",
    "inv_adapters = {}\n",
    "# For invertible adapters\n",
    "# group 1: adapter name\n",
    "# group 2: F/G identifier\n",
    "# group 3: 0/2 layer number\n",
    "# group 4: projection weight/bias\n",
    "inv_pattern = \"roberta\\.invertible_adapters\\.(\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\"\n",
    "for key in model.state_dict().keys():\n",
    "    match = re.search(pattern, key)\n",
    "    if match:\n",
    "        layer_num = str(match.group(1))\n",
    "        if layer_num not in organized_layers:\n",
    "            organized_layers[layer_num] = {}\n",
    "        adapter_name = match.group(2)\n",
    "        projection = match.group(3)\n",
    "        projection_type = match.group(4)\n",
    "        # print(f\"Layer: {layer_num}, Adapter: {adapter_name}, Projection: {projection}, Type: {projection_type}\")\n",
    "        if projection not in organized_layers[layer_num]:\n",
    "            organized_layers[layer_num][projection] = {}\n",
    "        if projection_type not in organized_layers[layer_num][projection]:\n",
    "            organized_layers[layer_num][projection][projection_type] = []\n",
    "        organized_layers[layer_num][projection][projection_type].append(key)\n",
    "    inv_match = re.search(inv_pattern, key)\n",
    "    if inv_match:\n",
    "        adapter_name = inv_match.group(1)\n",
    "        identifier = inv_match.group(2)\n",
    "        layer_num = inv_match.group(3)\n",
    "        projection_type = inv_match.group(4)\n",
    "        if identifier not in inv_adapters:\n",
    "            inv_adapters[identifier] = {}\n",
    "        if layer_num not in inv_adapters[identifier]:\n",
    "            inv_adapters[identifier][layer_num] = {}\n",
    "        if projection_type not in inv_adapters[identifier][layer_num]:\n",
    "            inv_adapters[identifier][layer_num][projection_type] = []\n",
    "        inv_adapters[identifier][layer_num][projection_type].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f65f2b3dd8cadf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T12:15:51.336206800Z",
     "start_time": "2025-05-01T12:15:51.311066100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# we now average the weights and biases of all layers over all adapters\n",
    "new_state_dict = OrderedDict()\n",
    "# to ensure we don't get problems, we check the config of all adapters\n",
    "all_adapters = list(model.adapters_config.adapters.keys())\n",
    "config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "config = model.adapters_config.config_map[config_id]\n",
    "for i in range(1, len(all_adapters)):\n",
    "    config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "    config_i = model.adapters_config.config_map[config_id]\n",
    "    assert config == config_i, (\n",
    "        f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "    )\n",
    "\n",
    "# if no problem, we go to the next step\n",
    "for layer_num, projections in organized_layers.items():\n",
    "    for projection, types in projections.items():\n",
    "        for projection_type, keys in types.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_weight = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = avg_weight\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = avg_weight\n",
    "\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_bias = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = avg_bias\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = avg_bias\n",
    "for identifier, layer_num in inv_adapters.items():\n",
    "    for layer_num, projections in layer_num.items():\n",
    "        for projection_type, keys in projections.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_weight = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = avg_weight\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                # avg_bias = (2/3) * sd[keys[0]] + (1/3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = avg_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4f9dcba03239b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:16.080286200Z",
     "start_time": "2025-04-25T08:57:15.932158300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have config saved from the last step, we create a new one in the same form\n",
    "if \"joined_adapter\" in model.adapters_config.adapters.keys():\n",
    "    # remove the old one\n",
    "    model.delete_adapter(\"joined_adapter\")\n",
    "model.add_adapter(\"joined_adapter\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bfa3a168ce9b9df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:23.271574400Z",
     "start_time": "2025-04-25T08:57:23.226464300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "    if \"joined_adapter\" in name and name in new_state_dict:\n",
    "        param.data.copy_(new_state_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "339ea2934154139d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:57:29.222230100Z",
     "start_time": "2025-04-25T08:57:29.130983500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleDict(\n  (joined_adapter): NICECouplingBlock(\n    (F): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n    (G): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in list(model.adapters_config.adapters.keys()):\n",
    "    if key != \"joined_adapter\":\n",
    "        model.delete_adapter(key)\n",
    "model.roberta.invertible_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ef990023bdf7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:18:27.706772400Z",
     "start_time": "2025-04-25T08:18:27.640331100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"./trained_adapters/mono/huge_avg_adapter\", \"huge_avg_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c10add83e138ca23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:41.801435Z",
     "start_time": "2025-04-23T20:14:41.796737500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we evaluated the adapter (along with de and en) on ner in another script\n",
    "import json\n",
    "\n",
    "results = json.load(open(\"methods/eval_dict_joined.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116e1fa2605c1c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:46.511129300Z",
     "start_time": "2025-04-23T20:14:46.507374900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss, avg en/de: 0.467184379696846, joined: 0.4572905898094177\n",
      "eval_model_preparation_time, avg en/de: 0.0086, joined: 0.006\n",
      "eval_precision, avg en/de: 0.5284809848704373, joined: 0.5575268817204301\n",
      "eval_recall, avg en/de: 0.7186234817813766, joined: 0.6997300944669366\n",
      "eval_f1, avg en/de: 0.6085899656003557, joined: 0.6205864751645721\n",
      "eval_accuracy, avg en/de: 0.8567322573513155, joined: 0.8606566438204731\n",
      "eval_runtime, avg en/de: 4.626099999999999, joined: 4.6817\n",
      "eval_samples_per_second, avg en/de: 216.16500000000002, joined: 213.599\n",
      "eval_steps_per_second, avg en/de: 27.0205, joined: 26.7\n"
     ]
    }
   ],
   "source": [
    "for (name, de), (_, en), (_, joined) in zip(\n",
    "    results[\"de\"].items(), results[\"en\"].items(), results[\"joined_adapter\"].items()\n",
    "):\n",
    "    print(f\"{name}, avg en/de: {(en + de) / 2}, joined: {joined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f7ecc3812d3b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:08:58.016222400Z",
     "start_time": "2025-04-25T08:08:57.665399600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# Fetch all AdapterHub xlm-roberta-base adapters\n",
    "models = api.list_models(author=\"AdapterHub\", library=\"adapter-transformers\", search=\"xlm-roberta-base-\")\n",
    "# we print all found models\n",
    "\n",
    "to_load = {\n",
    "    m.modelId: m.modelId.split(\"xlm-roberta-base-\")[-1].rsplit(\"-wiki_pfeiffer\", 1)[0]\n",
    "    for m in models\n",
    "    if m.modelId.startswith(\"AdapterHub/xlm-roberta-base-\") and m.modelId.endswith(\"-wiki_pfeiffer\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8300369a4d6da78f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T08:38:19.851726700Z",
     "start_time": "2025-04-25T08:38:02.071706200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452e37b101c242a79ccc9e96edfdd0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274360ba76104d27b3111c9e2c6f6060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9988603b604433be5c6882f581f60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12c90b2c7dc48f5ab67340774604126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435a841d8b824c7db839c214e4d73f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5709fe7b45c440e4af905da94df8789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7de4b1634f4c77a82bae61c59317a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d87d38da3b045b0b15ad3d243d15add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbc121e199c46f2a3afe96ffdd0c1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ffc85411e544c795048c0a604132f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0c5d00792b4dbabaea37c414074e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d7ae558d004eb3828fdffa767f8881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c7656e99d1467e8e8212608f510185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e56970c81642a0a920318c99d60e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f22bcc6c9a48aa9f704c41c2a740f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3a7b79db074625a6c0c030d1c68f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f31891bdc64f098dc8b7d6192145b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40b1b5667cd45c2ba29cf40ec1e4172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13773252e60248e69e07b36d64a2359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b79fa02e5b241e1b44689a914d82701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3e438b9fac482ea22f0439e0d3c869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7178c091d0fc4ba192642037d810a799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aee9b766a624ec6a8aaf9c287a42620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce91d3fde3ff4525a2d64c4069cd87a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c46f7c7f6949eaa4b984cbe6cdd9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751233abdc5046a09286f8bf23ab0f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990b77bfa51a4113a5676c840e4a8657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c3d79e2204dd7bb16bbd31e53d3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32280db6310849ce9459c1a6a67701c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, Stack\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"xlm-roberta-base\")\n",
    "didnt_load = []\n",
    "for link, id in to_load.items():\n",
    "    try:\n",
    "        model.load_adapter(link, load_as=id)\n",
    "    except OSError:\n",
    "        print(f\"Could not load {link}\")\n",
    "        didnt_load.append(link)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cbaa3c45636de66",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def merge_loaded_adapters(model, merge_adapter_name=\"joined_adapter\"):\n",
    "    def check_compatibility():\n",
    "        # to ensure we don't get problems, we check the config of all adapters\n",
    "        all_adapters = list(model.adapters_config.adapters.keys())\n",
    "        config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "        config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "        for i in range(1, len(all_adapters)):\n",
    "            config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "            config_i = model.adapters_config.config_map[config_id]\n",
    "            assert config == config_i, (\n",
    "                f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "            )\n",
    "\n",
    "    check_compatibility()\n",
    "\n",
    "    organized_layers = {}\n",
    "    # for each layer:\n",
    "    # group 1: layer number\n",
    "    # group 2: adapter name\n",
    "    # group 3: projection\n",
    "    # group 4: projection weight/bias\n",
    "    pattern = \"roberta\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\"\n",
    "\n",
    "    inv_adapters = {}\n",
    "    # For invertible adapters\n",
    "    # group 1: adapter name\n",
    "    # group 2: F/G identifier\n",
    "    # group 3: 0/2 layer number\n",
    "    # group 4: projection weight/bias\n",
    "    inv_pattern = \"roberta\\.invertible_adapters\\.(\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\"\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        layer_match = re.search(pattern, key)\n",
    "        if layer_match:\n",
    "            layer_num = layer_match.group(1)\n",
    "            # adapter_name = layer_match.group(2)\n",
    "            projection = layer_match.group(3)\n",
    "            projection_type = layer_match.group(4)\n",
    "            # print(f\"Layer: {layer_num}, Adapter: {adapter_name}, Projection: {projection}, Type: {projection_type}\")\n",
    "            if layer_num not in organized_layers:\n",
    "                organized_layers[layer_num] = {}\n",
    "            if projection not in organized_layers[layer_num]:\n",
    "                organized_layers[layer_num][projection] = {}\n",
    "            if projection_type not in organized_layers[layer_num][projection]:\n",
    "                organized_layers[layer_num][projection][projection_type] = []\n",
    "            organized_layers[layer_num][projection][projection_type].append(key)\n",
    "\n",
    "        inv_match = re.search(inv_pattern, key)\n",
    "        if inv_match:\n",
    "            # adapter_name = inv_match.group(1)\n",
    "            identifier = inv_match.group(2)\n",
    "            layer_num = inv_match.group(3)\n",
    "            projection_type = inv_match.group(4)\n",
    "            if identifier not in inv_adapters:\n",
    "                inv_adapters[identifier] = {}\n",
    "            if layer_num not in inv_adapters[identifier]:\n",
    "                inv_adapters[identifier][layer_num] = {}\n",
    "            if projection_type not in inv_adapters[identifier][layer_num]:\n",
    "                inv_adapters[identifier][layer_num][projection_type] = []\n",
    "            inv_adapters[identifier][layer_num][projection_type].append(key)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    # if no problem, we go to the next step\n",
    "    for layer_num, projections in organized_layers.items():\n",
    "        for projection, types in projections.items():\n",
    "            for projection_type, keys in types.items():\n",
    "                result = sum([sd[key] for key in keys]) / len(keys)\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.{projection_type}\"\n",
    "                    ] = result\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.{projection_type}\"\n",
    "                    ] = result\n",
    "\n",
    "    for identifier, layer_num in inv_adapters.items():\n",
    "        for layer_num, projections in layer_num.items():\n",
    "            for projection_type, keys in projections.items():\n",
    "                result = sum([sd[key] for key in keys]) / len(keys)\n",
    "                new_state_dict[\n",
    "                    f\"roberta.invertible_adapters.joined_adapter.{identifier}.{layer_num}.{projection_type}\"\n",
    "                ] = result\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "37dc81ad7cf9eba6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def improved_merge_loaded_adapters(\n",
    "    model, merge_adapter_name=\"joined_adapter\", weights=None, delete_other=False, patterns=False, model_type=\"roberta\"\n",
    "):\n",
    "    # to ensure we don't get problems, we check the config of all adapters\n",
    "    all_adapters = list(model.adapters_config.adapters.keys())\n",
    "    config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "    config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "    for i in range(1, len(all_adapters)):\n",
    "        config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "        config_i = model.adapters_config.config_map[config_id]\n",
    "        assert config == config_i, (\n",
    "            f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "        )\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1 / len(all_adapters)] * len(all_adapters)\n",
    "    if len(weights) != len(all_adapters):\n",
    "        raise ValueError(f\"Weights length {len(weights)} does not match number of adapters {len(all_adapters)}\")\n",
    "\n",
    "    if not patterns:\n",
    "        patterns = [\n",
    "            f\"{model_type}\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(?:\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\",\n",
    "            f\"{model_type}\\.invertible_adapters\\.(?:\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\",\n",
    "        ]\n",
    "    comp_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    organized_layers = {}\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        # we make a dictionary for each pattern\n",
    "        organized_layers[i] = {}\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        for i, pattern in enumerate(comp_patterns):\n",
    "            match = re.search(pattern, key)\n",
    "            if match:\n",
    "                one = match.group(1)\n",
    "                two = match.group(2)\n",
    "                three = match.group(3)\n",
    "                if one not in organized_layers[i]:\n",
    "                    organized_layers[i][one] = {}\n",
    "                if two not in organized_layers[i][one]:\n",
    "                    organized_layers[i][one][two] = {}\n",
    "                if three not in organized_layers[i][one][two]:\n",
    "                    organized_layers[i][one][two][three] = []\n",
    "                organized_layers[i][one][two][three].append(key)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    for i, one in organized_layers.items():\n",
    "        for one, two in one.items():\n",
    "            for two, three in two.items():\n",
    "                for three, keys in three.items():\n",
    "                    result = sum([sd[key] * weights[j] for j, key in enumerate(keys)])\n",
    "                    if two == \"adapter_down\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.0.{three}\"\n",
    "                        ] = result\n",
    "                    elif two == \"adapter_up\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.{three}\"\n",
    "                        ] = result\n",
    "                    else:\n",
    "                        # we are in the second pattern\n",
    "                        new_state_dict[f\"{model_type}.invertible_adapters.{merge_adapter_name}.{one}.{two}.{three}\"] = (\n",
    "                            result\n",
    "                        )\n",
    "\n",
    "    # we now load in the new model\n",
    "    if merge_adapter_name in model.adapters_config.adapters.keys():\n",
    "        # remove the old one\n",
    "        model.delete_adapter(merge_adapter_name)\n",
    "    model.add_adapter(merge_adapter_name, config=config)\n",
    "    for name, param in model.named_parameters():\n",
    "        # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "        if merge_adapter_name in name and name in new_state_dict:\n",
    "            param.data.copy_(new_state_dict[name])\n",
    "    if delete_other:\n",
    "        for key in list(model.adapters_config.adapters.keys()):\n",
    "            if key != merge_adapter_name:\n",
    "                model.delete_adapter(key)\n",
    "\n",
    "    # no need to return anything as the model is changed in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "46d4351398feeb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:43:58,490 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n",
      "2025-04-25 16:43:58,713 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model1 = copy.deepcopy(model.cpu())\n",
    "improved_merge_loaded_adapters(model1, delete_other=True)\n",
    "new_state_dict = merge_loaded_adapters(model)\n",
    "# we have config saved from the last step, we create a new one in the same form\n",
    "if \"joined_adapter\" in model.adapters_config.adapters.keys():\n",
    "    # remove the old one\n",
    "    model.delete_adapter(\"joined_adapter\")\n",
    "model.add_adapter(\"joined_adapter\", config=config)\n",
    "for name, param in model.named_parameters():\n",
    "    # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "    if \"joined_adapter\" in name and name in new_state_dict:\n",
    "        param.data.copy_(new_state_dict[name])\n",
    "for key in list(model.adapters_config.adapters.keys()):\n",
    "    if key != \"joined_adapter\":\n",
    "        model.delete_adapter(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4f1ef188a1bafe3f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are equal\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sd1 = model1.state_dict()\n",
    "sd2 = model.state_dict()\n",
    "for key in sd1.keys():\n",
    "    if key in sd2.keys():\n",
    "        assert torch.equal(sd1[key], sd2[key]), f\"Key {key} is not equal\"\n",
    "print(\"Models are equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "68042516a55ec3b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:13:18.333193200Z",
     "start_time": "2025-04-25T13:12:46.562082Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[117], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m glot \u001B[38;5;129;01min\u001B[39;00m not_found:\n\u001B[0;32m     21\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://glottolog.org/resource/languoid/id/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mglot\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 22\u001B[0m     response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(url)\n\u001B[0;32m     23\u001B[0m     soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# find the first h1 tag\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[0;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[0;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    679\u001B[0m     )\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    790\u001B[0m     conn,\n\u001B[0;32m    791\u001B[0m     method,\n\u001B[0;32m    792\u001B[0m     url,\n\u001B[0;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[0;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[0;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[0;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[0;32m    802\u001B[0m )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# Trigger any extra validation we need to do.\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 466\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_conn(conn)\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mconn\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[1;34m(self, conn)\u001B[0m\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;66;03m# Force connect early to allow us to validate the connection.\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[1;32m-> 1095\u001B[0m     conn\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001B[39;00m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mproxy_is_verified:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connection.py:693\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    692\u001B[0m     sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[1;32m--> 693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_conn()\n\u001B[0;32m    694\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[0;32m    695\u001B[0m     tls_in_tls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \n\u001B[0;32m    196\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[0;32m    200\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dns_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport),\n\u001B[0;32m    201\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout,\n\u001B[0;32m    202\u001B[0m         source_address\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address,\n\u001B[0;32m    203\u001B[0m         socket_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket_options,\n\u001B[0;32m    204\u001B[0m     )\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001B[0m, in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeError\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocationParseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, label empty or too long\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgetaddrinfo(host, port, family, socket\u001B[38;5;241m.\u001B[39mSOCK_STREAM):\n\u001B[0;32m     61\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[0;32m     62\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\socket.py:974\u001B[0m, in \u001B[0;36mgetaddrinfo\u001B[1;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;66;03m# We override this function since we want to translate the numeric family\u001B[39;00m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# and socket type values to enum constants.\u001B[39;00m\n\u001B[0;32m    973\u001B[0m addrlist \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 974\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m _socket\u001B[38;5;241m.\u001B[39mgetaddrinfo(host, port, family, \u001B[38;5;28mtype\u001B[39m, proto, flags):\n\u001B[0;32m    975\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[0;32m    976\u001B[0m     addrlist\u001B[38;5;241m.\u001B[39mappend((_intenum_converter(af, AddressFamily),\n\u001B[0;32m    977\u001B[0m                      _intenum_converter(socktype, SocketKind),\n\u001B[0;32m    978\u001B[0m                      proto, canonname, sa))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from qq import LanguageData, TagType\n",
    "from typdiv_sampling.evaluation import Evaluator\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "ld = LanguageData.from_db()\n",
    "\n",
    "evaluator = Evaluator()\n",
    "distances = evaluator.distances\n",
    "\n",
    "all_english = {}\n",
    "not_found = []\n",
    "for key in distances.keys():\n",
    "    try:\n",
    "        all_english[ld.get(key, tag_type=TagType.Glottocode).english_name] = key\n",
    "    except KeyError:\n",
    "        not_found.append(key)\n",
    "        continue\n",
    "\n",
    "for glot in not_found:\n",
    "    url = f\"https://glottolog.org/resource/languoid/id/{glot}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # find the first h1 tag\n",
    "    h3 = soup.find(\"h3\")\n",
    "    if h3:\n",
    "        span = h3.find(\"span\")\n",
    "        if span:\n",
    "            # print(span.text)  # This will print only the language name\n",
    "            all_english[span.text] = glot\n",
    "        else:\n",
    "            print(\"No <span> with class 'level-language' found.\")\n",
    "            print(h3.text)  # This will print the entire h3 text\n",
    "    else:\n",
    "        print(\"No <h3> tag found.\")\n",
    "\n",
    "\n",
    "def lookup_lang(snippet):\n",
    "    results = []\n",
    "    for lang in all_english.keys():\n",
    "        if re.search(snippet, lang):\n",
    "            results.append((lang, all_english[lang]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95b90d9ba2788e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:13:21.711707700Z",
     "start_time": "2025-04-25T13:13:21.704708700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ('Congo Swahili', 'cong1236') for Swahili\n",
      "Found ('Ayacucho Quechua', 'ayac1239') for Quechua\n",
      "Error: de - German - stan1295\n",
      "Found ('Standard Estonian', 'esto1258') for Estonian\n",
      "Found ('Eastern Bolivian Guaraní', 'east2555') for Guarani\n",
      "Found ('Baharna Arabic', 'baha1259') for Arabic\n",
      "Error: es - Spanish - stan1288\n",
      "Found ('Hakka Chinese', 'hakk1236') for Chinese\n",
      "Error: mhr - Eastern Mari - east2328\n",
      "Error: cdo - Min Dong Chinese - mind1253\n",
      "Error: xmf - Mingrelian - ming1252\n",
      "{'ar': 'baha1259',\n",
      " 'el': 'mode1248',\n",
      " 'en': 'stan1293',\n",
      " 'et': 'esto1258',\n",
      " 'gn': 'east2555',\n",
      " 'hi': 'hind1269',\n",
      " 'ht': 'hait1244',\n",
      " 'id': 'indo1316',\n",
      " 'ilo': 'ilok1237',\n",
      " 'is': 'icel1247',\n",
      " 'it': 'ital1282',\n",
      " 'ja': 'nucl1643',\n",
      " 'jv': 'java1254',\n",
      " 'mi': 'maor1246',\n",
      " 'my': 'nucl1310',\n",
      " 'qu': 'ayac1239',\n",
      " 'ru': 'russ1263',\n",
      " 'sw': 'cong1236',\n",
      " 'ta': 'tami1289',\n",
      " 'th': 'thai1261',\n",
      " 'tk': 'turk1304',\n",
      " 'tr': 'nucl1301',\n",
      " 'vi': 'viet1252',\n",
      " 'zh': 'hakk1236'}\n",
      "[('de', 'German', 'stan1295'), ('es', 'Spanish', 'stan1288'), ('mhr', 'Eastern Mari', 'east2328'), ('cdo', 'Min Dong Chinese', 'mind1253'), ('xmf', 'Mingrelian', 'ming1252')]\n"
     ]
    }
   ],
   "source": [
    "manuals = {\n",
    "    \"Arabic\": \"arab1267\",\n",
    "    \"Swahili\": \"swah1253\",\n",
    "    \"Bengali\": \"beng1282\",\n",
    "    \"Chinese\": \"mand1415\",\n",
    "    \"Persian\": \"west2369\",\n",
    "    \"Yoruba\": \"ilaa1246\",\n",
    "    \"Nepali\": \"nepa1254\",\n",
    "    \"Quechua\": \"cusc1236\",\n",
    "    \"Estonian\": \"esto1258\",\n",
    "    \"Guarani\": \"east2555\",\n",
    "}\n",
    "\n",
    "glots = {}\n",
    "probs = []\n",
    "\n",
    "for lang in to_load.values():\n",
    "    eng = ld.get(lang, tag_type=TagType.BCP_47_CODE).english_name\n",
    "    glot = ld.get(lang, tag_type=TagType.BCP_47_CODE).glottocode\n",
    "    # we need to find if glot is in distances\n",
    "    if not glot:\n",
    "        options = lookup_lang(eng[:-1])\n",
    "        # print(options)\n",
    "        if options:\n",
    "            verb_name, glot = options[0]\n",
    "            print(f\"Found {verb_name, glot} for {eng}\")\n",
    "            glots[lang] = glot\n",
    "        else:\n",
    "            print(f\"Error for {lang}: {eng} - {glot}\")\n",
    "            probs.append((lang, eng, glot))\n",
    "            continue\n",
    "    if glot not in distances.keys():\n",
    "        print(f\"Error: {lang} - {eng} - {glot}\")\n",
    "        probs.append((lang, eng, glot))\n",
    "        continue\n",
    "    if eng and glot:\n",
    "        glots[lang] = glot\n",
    "\n",
    "pprint(glots)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5d74f7272b6c66aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:35:32.934530200Z",
     "start_time": "2025-04-25T14:35:15.431742700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting to URIEL involves copying the files ['family_features.npz', 'features.npz', 'geocoord_features.npz'] into the data directory. Any files with the same name will be replaced. Continue? [Y/n] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:35:20,332 - root - INFO - Importing all databases....\n",
      "2025-04-25 16:35:20,335 - root - INFO - Importing updated SAPHON from \"saphon_data.csv\"....\n",
      "2025-04-25 16:35:21,840 - root - INFO - Updated SAPHON integration complete..\n",
      "2025-04-25 16:35:21,842 - root - INFO - Importing BDPROTO from \"bdproto_data.csv\"....\n",
      "2025-04-25 16:35:21,843 - root - INFO - Converting ISO 639-3 codes to Glottocodes....\n",
      "2025-04-25 16:35:22,236 - root - INFO - Conversion to Glottocodes complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[200], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m u\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m      4\u001B[0m u\u001B[38;5;241m.\u001B[39mset_cache(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 5\u001B[0m u\u001B[38;5;241m.\u001B[39mintegrate_databases()\n\u001B[0;32m      6\u001B[0m u\u001B[38;5;241m.\u001B[39mset_glottocodes()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:596\u001B[0m, in \u001B[0;36mURIELPlusDatabases.integrate_databases\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m db, integrate_method \u001B[38;5;129;01min\u001B[39;00m databases\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m db \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msources[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 596\u001B[0m         integrate_method()\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minferred_features()\n\u001B[0;32m    602\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll databases integration complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:364\u001B[0m, in \u001B[0;36mURIELPlusDatabases.integrate_bdproto\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache:\n\u001B[0;32m    360\u001B[0m     np\u001B[38;5;241m.\u001B[39msavez(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcur_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabase\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles[\u001B[38;5;241m1\u001B[39m]),\n\u001B[0;32m    361\u001B[0m              feats\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeats[\u001B[38;5;241m1\u001B[39m], data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m1\u001B[39m], langs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m1\u001B[39m], sources\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msources[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m--> 364\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_phylogeny_vectors()\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_geocoord_vectors()\n\u001B[0;32m    368\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBDPROTO integration complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\urielplus\\urielplus_databases.py:134\u001B[0m, in \u001B[0;36mURIELPlusDatabases._calculate_phylogeny_vectors\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m], l)\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]))])\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m], l)\n\u001B[0;32m    136\u001B[0m new_lang_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlangs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m l)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\numpy\\_core\\shape_base.py:291\u001B[0m, in \u001B[0;36mvstack\u001B[1;34m(tup, dtype, casting)\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arrs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    290\u001B[0m     arrs \u001B[38;5;241m=\u001B[39m (arrs,)\n\u001B[1;32m--> 291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _nx\u001B[38;5;241m.\u001B[39mconcatenate(arrs, \u001B[38;5;241m0\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype, casting\u001B[38;5;241m=\u001B[39mcasting)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from urielplus import urielplus\n",
    "\n",
    "u = urielplus.URIELPlus()\n",
    "u.reset()\n",
    "u.set_cache(True)\n",
    "u.integrate_databases()\n",
    "u.set_glottocodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7090d376cb17f78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:46:10.392463Z",
     "start_time": "2025-04-25T14:46:10.359076100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def typological_approximation_dev(target, languages):\n",
    "    \"\"\"\n",
    "    This function takes a target language and a list of languages.\n",
    "    It weights the other languages depending on their closeness to the target language.\n",
    "    \"\"\"\n",
    "    # 1. check if all languages are in the distances\n",
    "    for lang in languages:\n",
    "        if glots[lang] not in distances.keys():\n",
    "            print(f\"Language {lang}, {glots[lang]} not in distances\")\n",
    "\n",
    "    # 2. retrieve closeness score of all languages to target language\n",
    "    weights = []\n",
    "    for lang in languages:\n",
    "        # get the distance\n",
    "        try:\n",
    "            dist = 1 - u.new_distance(\"featural\", [glots[lang], target])\n",
    "            print(f\"Distance {lang} to {target}: {dist}\")\n",
    "        except SystemExit:\n",
    "            print(f\"Error: {lang} - {glots[lang]} - {target}\")\n",
    "            dist = 0\n",
    "        weights.append(dist)\n",
    "\n",
    "    # 1. softmax over weights\n",
    "    print(f\"Weights before softmax: {weights}\")\n",
    "    weights = torch.softmax(torch.tensor(weights), dim=0)\n",
    "    # we need to convert to list\n",
    "    weights = weights.tolist()\n",
    "    print(f\"Weights after softmax: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e16022d51bc3b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "588490bb28529704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:48:23.437832800Z",
     "start_time": "2025-04-25T14:48:23.094231800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:48:23,105 - root - INFO - In new_distance, calculated angular distance for featural with thai1261 and afri1274: 0.011420488357543945 seconds\n",
      "2025-04-25 16:48:23,119 - root - INFO - In new_distance, calculated angular distance for featural with nucl1310 and afri1274: 0.010793685913085938 seconds\n",
      "2025-04-25 16:48:23,129 - root - INFO - In new_distance, calculated angular distance for featural with hind1269 and afri1274: 0.008425235748291016 seconds\n",
      "2025-04-25 16:48:23,141 - root - INFO - In new_distance, calculated angular distance for featural with ilok1237 and afri1274: 0.009675741195678711 seconds\n",
      "2025-04-25 16:48:23,150 - root - INFO - In new_distance, calculated angular distance for featural with hait1244 and afri1274: 0.00613713264465332 seconds\n",
      "2025-04-25 16:48:23,160 - root - INFO - In new_distance, calculated angular distance for featural with nucl1301 and afri1274: 0.008888959884643555 seconds\n",
      "2025-04-25 16:48:23,172 - root - INFO - In new_distance, calculated angular distance for featural with maor1246 and afri1274: 0.009157180786132812 seconds\n",
      "2025-04-25 16:48:23,186 - root - INFO - In new_distance, calculated angular distance for featural with viet1252 and afri1274: 0.010975837707519531 seconds\n",
      "2025-04-25 16:48:23,200 - root - INFO - In new_distance, calculated angular distance for featural with icel1247 and afri1274: 0.011210441589355469 seconds\n",
      "2025-04-25 16:48:23,213 - root - INFO - In new_distance, calculated angular distance for featural with ital1282 and afri1274: 0.010046005249023438 seconds\n",
      "2025-04-25 16:48:23,222 - root - INFO - In new_distance, calculated angular distance for featural with tami1289 and afri1274: 0.007052898406982422 seconds\n",
      "2025-04-25 16:48:23,231 - root - INFO - In new_distance, calculated angular distance for featural with java1254 and afri1274: 0.006459712982177734 seconds\n",
      "2025-04-25 16:48:23,244 - root - INFO - In new_distance, calculated angular distance for featural with nucl1643 and afri1274: 0.010086774826049805 seconds\n",
      "2025-04-25 16:48:23,248 - root - ERROR - No shared featural features between cong1236 and afri1274 for which the two languages have information.\n",
      "Unable to calculate featural distance.\n",
      "2025-04-25 16:48:23,260 - root - INFO - In new_distance, calculated angular distance for featural with ayac1239 and afri1274: 0.00893259048461914 seconds\n",
      "2025-04-25 16:48:23,273 - root - INFO - In new_distance, calculated angular distance for featural with mode1248 and afri1274: 0.01037907600402832 seconds\n",
      "2025-04-25 16:48:23,284 - root - INFO - In new_distance, calculated angular distance for featural with esto1258 and afri1274: 0.008218765258789062 seconds\n",
      "2025-04-25 16:48:23,293 - root - INFO - In new_distance, calculated angular distance for featural with russ1263 and afri1274: 0.007620573043823242 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance th to afri1274: 0.5305\n",
      "Distance my to afri1274: 0.5578\n",
      "Distance hi to afri1274: 0.5374\n",
      "Distance ilo to afri1274: 0.5095\n",
      "Distance ht to afri1274: 0.5662\n",
      "Distance tr to afri1274: 0.4901\n",
      "Distance mi to afri1274: 0.5587\n",
      "Distance vi to afri1274: 0.5451\n",
      "Distance is to afri1274: 0.4266\n",
      "Distance it to afri1274: 0.4516\n",
      "Distance ta to afri1274: 0.5581\n",
      "Distance jv to afri1274: 0.4661\n",
      "Distance ja to afri1274: 0.5726\n",
      "Error: sw - cong1236 - afri1274\n",
      "Distance qu to afri1274: 0.6357\n",
      "Distance el to afri1274: 0.5302\n",
      "Distance et to afri1274: 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:48:23,305 - root - INFO - In new_distance, calculated angular distance for featural with east2555 and afri1274: 0.009303569793701172 seconds\n",
      "2025-04-25 16:48:23,316 - root - INFO - In new_distance, calculated angular distance for featural with indo1316 and afri1274: 0.008577585220336914 seconds\n",
      "2025-04-25 16:48:23,328 - root - INFO - In new_distance, calculated angular distance for featural with stan1293 and afri1274: 0.008578062057495117 seconds\n",
      "2025-04-25 16:48:23,333 - root - ERROR - No shared featural features between baha1259 and afri1274 for which the two languages have information.\n",
      "Unable to calculate featural distance.\n",
      "2025-04-25 16:48:23,343 - root - INFO - In new_distance, calculated angular distance for featural with turk1304 and afri1274: 0.007827520370483398 seconds\n",
      "2025-04-25 16:48:23,354 - root - INFO - In new_distance, calculated angular distance for featural with hakk1236 and afri1274: 0.007961750030517578 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance ru to afri1274: 0.4596\n",
      "Distance gn to afri1274: 0.5552\n",
      "Distance id to afri1274: 0.5453\n",
      "Distance en to afri1274: 0.4907\n",
      "Error: ar - baha1259 - afri1274\n",
      "Distance tk to afri1274: 0.3918\n",
      "Distance zh to afri1274: 0.5288\n",
      "Weights before softmax: [np.float64(0.5305), np.float64(0.5578), np.float64(0.5374), np.float64(0.5095), np.float64(0.5662), np.float64(0.4901), np.float64(0.5587), np.float64(0.5451), np.float64(0.4266), np.float64(0.4516), np.float64(0.5581), np.float64(0.4661), np.float64(0.5726), 0, np.float64(0.6357), np.float64(0.5302), np.float64(0.2952), np.float64(0.4596), np.float64(0.5552), np.float64(0.5453), np.float64(0.4907), 0, np.float64(0.3918), np.float64(0.5288)]\n",
      "Weights after softmax: [0.043923663689886254, 0.0451392976126194, 0.04422778498120124, 0.04301088447850758, 0.045520064695405295, 0.0421845150209015, 0.04517994126737195, 0.04456965342997089, 0.03958907558236617, 0.04059127780218449, 0.045152841433374724, 0.041184139187974006, 0.04581232735236898, 0.025840763122152884, 0.04879623757770814, 0.04391048856714651, 0.0347143522541419, 0.04091731041621935, 0.04502208787751038, 0.04457856825210938, 0.04220983332464562, 0.025840763122152884, 0.03823507205672382, 0.0438490568953566]\n"
     ]
    }
   ],
   "source": [
    "weights = typological_approximation_dev(\"afri1274\", list(glots.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf49c09fa9227fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Limiting the activated adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6dec64f6f7b85ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:01.153331700Z",
     "start_time": "2025-05-01T13:33:01.136100600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_loaded_adapters(\n",
    "    model, merge_adapter_name=\"joined_adapter\", weights=None, delete_other=False, patterns=False, model_type=\"roberta\"\n",
    "):\n",
    "    # to ensure we don't get problems, we check the config of all adapters\n",
    "    all_adapters = list(model.adapters_config.adapters.keys())\n",
    "    config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "    config = model.adapters_config.config_map[config_id]\n",
    "\n",
    "    for i in range(1, len(all_adapters)):\n",
    "        config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "        config_i = model.adapters_config.config_map[config_id]\n",
    "        assert config == config_i, (\n",
    "            f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "        )\n",
    "\n",
    "    if weights is None or weights == {}:\n",
    "        weights = {adapter: 1 / len(all_adapters) for adapter in all_adapters}\n",
    "    print(\"weights:\", weights)\n",
    "    if not patterns:\n",
    "        patterns = [\n",
    "            f\"{model_type}\\.encoder\\.layer\\.(?P<one>[\\d\\w]+)\\.output\\.adapters\\.(?P<adapter>\\w+)\\.(?P<two>\\w+)(?:\\.0)?\\.(?P<three>\\w+)\",\n",
    "            f\"{model_type}\\.invertible_adapters\\.(?P<adapter>\\w+)\\.(?P<one>\\w+)\\.(?P<two>\\d)\\.(?P<three>\\w+)\",\n",
    "        ]\n",
    "    comp_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    organized_layers = {}\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        # we make a dictionary for each pattern\n",
    "        organized_layers[i] = {}\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        for i, pattern in enumerate(comp_patterns):\n",
    "            match = re.search(pattern, key)\n",
    "            if match:\n",
    "                one = match.group(\"one\")\n",
    "                two = match.group(\"two\")\n",
    "                three = match.group(\"three\")\n",
    "                adapter_name = match.group(\"adapter\")\n",
    "                if adapter_name not in weights.keys():\n",
    "                    print(f\"Adapter {adapter_name} not in weights\")\n",
    "                    continue\n",
    "                if one not in organized_layers[i]:\n",
    "                    organized_layers[i][one] = {}\n",
    "                if two not in organized_layers[i][one]:\n",
    "                    organized_layers[i][one][two] = {}\n",
    "                if three not in organized_layers[i][one][two]:\n",
    "                    organized_layers[i][one][two][three] = []\n",
    "                organized_layers[i][one][two][three].append((key, adapter_name))\n",
    "    pprint(organized_layers)\n",
    "    new_state_dict = OrderedDict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    for i, one in organized_layers.items():\n",
    "        for one, two in one.items():\n",
    "            for two, three in two.items():\n",
    "                for three, keys in three.items():\n",
    "                    # result = sum([sd[layer] * weights[adapter_name] for layer, adapter_name in keys])\n",
    "                    result = 0\n",
    "                    for layer, adapter_name in keys:\n",
    "                        print(layer, adapter_name, weights[adapter_name])\n",
    "                        if adapter_name in weights.keys():\n",
    "                            result += sd[layer] * weights[adapter_name]\n",
    "\n",
    "                    if two == \"adapter_down\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.0.{three}\"\n",
    "                        ] = result\n",
    "                    elif two == \"adapter_up\":\n",
    "                        new_state_dict[\n",
    "                            f\"{model_type}.encoder.layer.{one}.output.adapters.{merge_adapter_name}.{two}.{three}\"\n",
    "                        ] = result\n",
    "                    else:\n",
    "                        # we are in the second pattern\n",
    "                        new_state_dict[f\"{model_type}.invertible_adapters.{merge_adapter_name}.{one}.{two}.{three}\"] = (\n",
    "                            result\n",
    "                        )\n",
    "\n",
    "    # we now load in the new model\n",
    "    if merge_adapter_name in model.adapters_config.adapters.keys():\n",
    "        # remove the old one\n",
    "        model.delete_adapter(merge_adapter_name)\n",
    "    model.add_adapter(merge_adapter_name, config=config)\n",
    "    for name, param in model.named_parameters():\n",
    "        # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "        if merge_adapter_name in name and name in new_state_dict:\n",
    "            param.data.copy_(new_state_dict[name])\n",
    "    if delete_other:\n",
    "        for key in list(model.adapters_config.adapters.keys()):\n",
    "            if key != merge_adapter_name:\n",
    "                model.delete_adapter(key)\n",
    "\n",
    "\n",
    "def typological_approximation(target, glots, limit=None):\n",
    "    \"\"\"\n",
    "    This function takes a target language and a list of languages.\n",
    "    It weights the other languages depending on their closeness to the target language.\n",
    "    If limit is specified and is <1, it will remove all languages with a distance lower than limit.\n",
    "    If limit is specified and is >=1, it works as a top-k languages filter with the highest similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. retrieve closeness score of all languages to target language\n",
    "    weights = {}\n",
    "    for lang, glot in glots.items():\n",
    "        # get the distance\n",
    "        try:\n",
    "            dist = 1 - u.new_distance(\"featural\", [glot, target])\n",
    "            print(f\"Distance {lang} to {target}: {dist}\")\n",
    "        except SystemExit:\n",
    "            print(f\"Error: {lang} - {glot} - {target}\")\n",
    "            dist = 0\n",
    "        weights[lang] = dist\n",
    "    # we add basque\n",
    "    eu_glot = ld.get(\"eu\", tag_type=TagType.BCP_47_CODE).glottocode\n",
    "    dist = 1 - u.new_distance(\"featural\", [eu_glot, target])\n",
    "    print(f\"Distance Basque to {target}: {dist}\")\n",
    "    weights[\"eu\"] = dist\n",
    "    if limit:\n",
    "        if limit < 1:\n",
    "            for lang, dist in list(weights.items()):\n",
    "                if dist < limit:\n",
    "                    print(f\"Removing {lang} with distance {dist}\")\n",
    "                    del weights[lang]\n",
    "        else:  # we take the best n (limit) languages\n",
    "            n = min(limit, len(weights))\n",
    "            # we sort the weights\n",
    "            sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)\n",
    "            # we take the first n\n",
    "            weights = dict(sorted_weights[:n])\n",
    "\n",
    "    # 1. softmax over weights\n",
    "    print(f\"Weights before softmax: {weights}\")\n",
    "    soft_weights = torch.softmax(torch.tensor(list(weights.values())), dim=0)\n",
    "    # we need to convert to list\n",
    "    soft_weights = soft_weights.tolist()\n",
    "    # we zippedly print the keys and values\n",
    "    weights = {k: v for k, v in zip(weights.keys(), soft_weights)}\n",
    "    print(f\"Weights after softmax: {weights}\")\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_glots(to_load):\n",
    "    manuals = {\n",
    "        \"Arabic\": \"arab1267\",\n",
    "        \"Swahili\": \"swah1253\",\n",
    "        \"Bengali\": \"beng1282\",\n",
    "        \"Chinese\": \"mand1415\",\n",
    "        \"Persian\": \"west2369\",\n",
    "        \"Yoruba\": \"ilaa1246\",\n",
    "        \"Nepali\": \"nepa1254\",\n",
    "        \"Quechua\": \"cusc1236\",\n",
    "        \"Estonian\": \"esto1258\",\n",
    "        \"Guarani\": \"east2555\",\n",
    "    }\n",
    "\n",
    "    glots = {}\n",
    "    probs = []\n",
    "\n",
    "    for lang in to_load.values():\n",
    "        eng = ld.get(lang, tag_type=TagType.BCP_47_CODE).english_name\n",
    "        glot = ld.get(lang, tag_type=TagType.BCP_47_CODE).glottocode\n",
    "        # we need to find if glot is in distances\n",
    "        if not glot:\n",
    "            if eng in manuals.keys():\n",
    "                glot = manuals[eng]\n",
    "        if eng and glot:\n",
    "            glots[lang] = glot\n",
    "        else:\n",
    "            probs.append(lang)\n",
    "\n",
    "    print(\"no glottocodes found for these languages: \", probs)\n",
    "    print(\"removing them from further consideration\")\n",
    "    for prob in probs:\n",
    "        del to_load[prob]\n",
    "        # happens in-place\n",
    "    return glots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "957a4b58a9ff3384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:02.345519900Z",
     "start_time": "2025-05-01T13:33:02.340409100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no glottocodes found for these languages:  []\n",
      "removing them from further consideration\n"
     ]
    }
   ],
   "source": [
    "to_load = {\n",
    "    \"English\": \"en\",\n",
    "    \"German\": \"de\",\n",
    "    \"Spanish\": \"es\",\n",
    "}\n",
    "glots = get_glots(to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a8b6d91b4f1de5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:08:44.122698200Z",
     "start_time": "2025-05-01T14:08:44.070090700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 16:08:44,078 - root - INFO - In new_distance, calculated angular distance for featural with stan1293 and dutc1256: 0.009697198867797852 seconds\n",
      "2025-05-01 16:08:44,086 - root - INFO - In new_distance, calculated angular distance for featural with stan1295 and dutc1256: 0.006561279296875 seconds\n",
      "2025-05-01 16:08:44,094 - root - INFO - In new_distance, calculated angular distance for featural with stan1288 and dutc1256: 0.007330417633056641 seconds\n",
      "2025-05-01 16:08:44,104 - root - INFO - In new_distance, calculated angular distance for featural with basq1248 and dutc1256: 0.008893966674804688 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance en to dutc1256: 0.5473\n",
      "Distance de to dutc1256: 0.6954\n",
      "Distance es to dutc1256: 0.46630000000000005\n",
      "Distance Basque to dutc1256: 0.393\n",
      "Weights before softmax: {'de': np.float64(0.6954), 'en': np.float64(0.5473), 'es': np.float64(0.46630000000000005)}\n",
      "Weights after softmax: {'de': 0.3762802161893739, 'en': 0.3244833164247225, 'es': 0.29923646738590365}\n",
      "Best adapter: de\n"
     ]
    }
   ],
   "source": [
    "weights = typological_approximation(\"dutc1256\", glots, limit=3)\n",
    "sorted_ad = max(weights.items(), key=lambda x: x[1])\n",
    "closest_adapter = max(weights, key=weights.get)\n",
    "print(f\"Best adapter: {closest_adapter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a7d1ca7fae59d19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:33:09.769830800Z",
     "start_time": "2025-05-01T13:33:09.616229800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 15:33:09,644 - adapters.configuration.model_adapters_config - INFO - Adding adapter 'joined_adapter'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: {'de': 0.3333333333333333, 'en': 0.3333333333333333, 'eu': 0.3333333333333333}\n",
      "{0: {'0': {'adapter_down': {'bias': [('roberta.encoder.layer.0.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.0.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.0.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.0.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.0.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.0.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.0.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.0.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '1': {'adapter_down': {'bias': [('roberta.encoder.layer.1.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.1.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.1.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.1.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.1.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.1.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.1.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.1.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '10': {'adapter_down': {'bias': [('roberta.encoder.layer.10.output.adapters.de.adapter_down.0.bias',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.en.adapter_down.0.bias',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.bias',\n",
      "                                       'eu')],\n",
      "                             'weight': [('roberta.encoder.layer.10.output.adapters.de.adapter_down.0.weight',\n",
      "                                         'de'),\n",
      "                                        ('roberta.encoder.layer.10.output.adapters.en.adapter_down.0.weight',\n",
      "                                         'en'),\n",
      "                                        ('roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.weight',\n",
      "                                         'eu')]},\n",
      "            'adapter_up': {'bias': [('roberta.encoder.layer.10.output.adapters.de.adapter_up.bias',\n",
      "                                     'de'),\n",
      "                                    ('roberta.encoder.layer.10.output.adapters.en.adapter_up.bias',\n",
      "                                     'en'),\n",
      "                                    ('roberta.encoder.layer.10.output.adapters.eu.adapter_up.bias',\n",
      "                                     'eu')],\n",
      "                           'weight': [('roberta.encoder.layer.10.output.adapters.de.adapter_up.weight',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.en.adapter_up.weight',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.10.output.adapters.eu.adapter_up.weight',\n",
      "                                       'eu')]}},\n",
      "     '11': {'adapter_down': {'bias': [('roberta.encoder.layer.11.output.adapters.de.adapter_down.0.bias',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.en.adapter_down.0.bias',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.bias',\n",
      "                                       'eu')],\n",
      "                             'weight': [('roberta.encoder.layer.11.output.adapters.de.adapter_down.0.weight',\n",
      "                                         'de'),\n",
      "                                        ('roberta.encoder.layer.11.output.adapters.en.adapter_down.0.weight',\n",
      "                                         'en'),\n",
      "                                        ('roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.weight',\n",
      "                                         'eu')]},\n",
      "            'adapter_up': {'bias': [('roberta.encoder.layer.11.output.adapters.de.adapter_up.bias',\n",
      "                                     'de'),\n",
      "                                    ('roberta.encoder.layer.11.output.adapters.en.adapter_up.bias',\n",
      "                                     'en'),\n",
      "                                    ('roberta.encoder.layer.11.output.adapters.eu.adapter_up.bias',\n",
      "                                     'eu')],\n",
      "                           'weight': [('roberta.encoder.layer.11.output.adapters.de.adapter_up.weight',\n",
      "                                       'de'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.en.adapter_up.weight',\n",
      "                                       'en'),\n",
      "                                      ('roberta.encoder.layer.11.output.adapters.eu.adapter_up.weight',\n",
      "                                       'eu')]}},\n",
      "     '2': {'adapter_down': {'bias': [('roberta.encoder.layer.2.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.2.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.2.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.2.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.2.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.2.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.2.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.2.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '3': {'adapter_down': {'bias': [('roberta.encoder.layer.3.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.3.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.3.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.3.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.3.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.3.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.3.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.3.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '4': {'adapter_down': {'bias': [('roberta.encoder.layer.4.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.4.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.4.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.4.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.4.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.4.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.4.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.4.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '5': {'adapter_down': {'bias': [('roberta.encoder.layer.5.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.5.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.5.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.5.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.5.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.5.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.5.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.5.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '6': {'adapter_down': {'bias': [('roberta.encoder.layer.6.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.6.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.6.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.6.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.6.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.6.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.6.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.6.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '7': {'adapter_down': {'bias': [('roberta.encoder.layer.7.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.7.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.7.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.7.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.7.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.7.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.7.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.7.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '8': {'adapter_down': {'bias': [('roberta.encoder.layer.8.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.8.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.8.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.8.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.8.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.8.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.8.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.8.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}},\n",
      "     '9': {'adapter_down': {'bias': [('roberta.encoder.layer.9.output.adapters.de.adapter_down.0.bias',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.en.adapter_down.0.bias',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.bias',\n",
      "                                      'eu')],\n",
      "                            'weight': [('roberta.encoder.layer.9.output.adapters.de.adapter_down.0.weight',\n",
      "                                        'de'),\n",
      "                                       ('roberta.encoder.layer.9.output.adapters.en.adapter_down.0.weight',\n",
      "                                        'en'),\n",
      "                                       ('roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.weight',\n",
      "                                        'eu')]},\n",
      "           'adapter_up': {'bias': [('roberta.encoder.layer.9.output.adapters.de.adapter_up.bias',\n",
      "                                    'de'),\n",
      "                                   ('roberta.encoder.layer.9.output.adapters.en.adapter_up.bias',\n",
      "                                    'en'),\n",
      "                                   ('roberta.encoder.layer.9.output.adapters.eu.adapter_up.bias',\n",
      "                                    'eu')],\n",
      "                          'weight': [('roberta.encoder.layer.9.output.adapters.de.adapter_up.weight',\n",
      "                                      'de'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.en.adapter_up.weight',\n",
      "                                      'en'),\n",
      "                                     ('roberta.encoder.layer.9.output.adapters.eu.adapter_up.weight',\n",
      "                                      'eu')]}}},\n",
      " 1: {'F': {'0': {'bias': [('roberta.invertible_adapters.de.F.0.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.F.0.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.F.0.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.F.0.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.F.0.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.F.0.weight',\n",
      "                             'eu')]},\n",
      "           '2': {'bias': [('roberta.invertible_adapters.de.F.2.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.F.2.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.F.2.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.F.2.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.F.2.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.F.2.weight',\n",
      "                             'eu')]}},\n",
      "     'G': {'0': {'bias': [('roberta.invertible_adapters.de.G.0.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.G.0.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.G.0.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.G.0.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.G.0.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.G.0.weight',\n",
      "                             'eu')]},\n",
      "           '2': {'bias': [('roberta.invertible_adapters.de.G.2.bias', 'de'),\n",
      "                          ('roberta.invertible_adapters.en.G.2.bias', 'en'),\n",
      "                          ('roberta.invertible_adapters.eu.G.2.bias', 'eu')],\n",
      "                 'weight': [('roberta.invertible_adapters.de.G.2.weight', 'de'),\n",
      "                            ('roberta.invertible_adapters.en.G.2.weight', 'en'),\n",
      "                            ('roberta.invertible_adapters.eu.G.2.weight',\n",
      "                             'eu')]}}}}\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.0.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.1.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.2.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.3.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.4.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.5.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.6.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.7.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.8.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.9.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.10.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_down.0.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_down.0.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_down.0.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_down.0.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_down.0.bias eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_up.weight de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_up.weight en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_up.weight eu 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.de.adapter_up.bias de 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.en.adapter_up.bias en 0.3333333333333333\n",
      "roberta.encoder.layer.11.output.adapters.eu.adapter_up.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.0.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.0.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.0.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.0.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.0.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.0.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.2.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.2.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.2.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.F.2.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.F.2.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.F.2.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.0.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.0.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.0.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.0.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.0.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.0.bias eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.2.weight de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.2.weight en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.2.weight eu 0.3333333333333333\n",
      "roberta.invertible_adapters.de.G.2.bias de 0.3333333333333333\n",
      "roberta.invertible_adapters.en.G.2.bias en 0.3333333333333333\n",
      "roberta.invertible_adapters.eu.G.2.bias eu 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "merge_loaded_adapters(model, merge_adapter_name=\"joined_adapter\", weights=weights)\n",
    "model.delete_adapter(\"joined_adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
